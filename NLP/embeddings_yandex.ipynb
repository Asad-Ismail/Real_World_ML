{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf308ae3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (3.8.1)\n",
      "Requirement already satisfied: gensim in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (4.3.2)\n",
      "Requirement already satisfied: bokeh in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (3.2.2)\n",
      "Requirement already satisfied: click in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from nltk) (8.1.6)\n",
      "Requirement already satisfied: joblib in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from nltk) (1.3.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from nltk) (2023.6.3)\n",
      "Requirement already satisfied: tqdm in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from nltk) (4.65.0)\n",
      "Requirement already satisfied: numpy>=1.18.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from gensim) (1.22.3)\n",
      "Requirement already satisfied: scipy>=1.7.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from gensim) (1.11.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from gensim) (6.4.0)\n",
      "Requirement already satisfied: Jinja2>=2.9 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from bokeh) (3.1.2)\n",
      "Requirement already satisfied: contourpy>=1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from bokeh) (1.1.0)\n",
      "Requirement already satisfied: packaging>=16.8 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from bokeh) (21.3)\n",
      "Requirement already satisfied: pandas>=1.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from bokeh) (2.0.3)\n",
      "Requirement already satisfied: pillow>=7.1.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from bokeh) (9.4.0)\n",
      "Requirement already satisfied: PyYAML>=3.10 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from bokeh) (6.0)\n",
      "Requirement already satisfied: tornado>=5.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from bokeh) (6.3.2)\n",
      "Requirement already satisfied: xyzservices>=2021.09.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from bokeh) (2023.7.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from Jinja2>=2.9->bokeh) (2.1.3)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from packaging>=16.8->bokeh) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pandas>=1.2->bokeh) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pandas>=1.2->bokeh) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pandas>=1.2->bokeh) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas>=1.2->bokeh) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade nltk gensim bokeh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4dab1235",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-10-05 22:01:01--  https://www.dropbox.com/s/obaitrix9jyu84r/quora.txt?dl=1\n",
      "Resolving www.dropbox.com (www.dropbox.com)... 162.125.6.18, 2620:100:601c:18::a27d:612\n",
      "Connecting to www.dropbox.com (www.dropbox.com)|162.125.6.18|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: /s/dl/obaitrix9jyu84r/quora.txt [following]\n",
      "--2023-10-05 22:01:01--  https://www.dropbox.com/s/dl/obaitrix9jyu84r/quora.txt\n",
      "Reusing existing connection to www.dropbox.com:443.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://uc0799962b5134539a886069c228.dl.dropboxusercontent.com/cd/0/get/CFDYpXhmS0vm8nGGf7ESGC6GCFVhdfkIDKaMJw7w5Ah2XvEi8fJF8xNA4DvKp0KvkuDGUzWlkabKgZbKKse16x1s4VkzcKMBouENPIIgIjhfu6hz3zYeS_xcX7WplL87rTw/file?dl=1# [following]\n",
      "--2023-10-05 22:01:01--  https://uc0799962b5134539a886069c228.dl.dropboxusercontent.com/cd/0/get/CFDYpXhmS0vm8nGGf7ESGC6GCFVhdfkIDKaMJw7w5Ah2XvEi8fJF8xNA4DvKp0KvkuDGUzWlkabKgZbKKse16x1s4VkzcKMBouENPIIgIjhfu6hz3zYeS_xcX7WplL87rTw/file?dl=1\n",
      "Resolving uc0799962b5134539a886069c228.dl.dropboxusercontent.com (uc0799962b5134539a886069c228.dl.dropboxusercontent.com)... 162.125.6.15, 2620:100:601c:15::a27d:60f\n",
      "Connecting to uc0799962b5134539a886069c228.dl.dropboxusercontent.com (uc0799962b5134539a886069c228.dl.dropboxusercontent.com)|162.125.6.15|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 33813903 (32M) [application/binary]\n",
      "Saving to: ‘./quora.txt’\n",
      "\n",
      "100%[======================================>] 33,813,903   198MB/s   in 0.2s   \n",
      "\n",
      "2023-10-05 22:01:02 (198 MB/s) - ‘./quora.txt’ saved [33813903/33813903]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# download the data:\n",
    "!wget https://www.dropbox.com/s/obaitrix9jyu84r/quora.txt?dl=1 -O ./quora.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9bdc612c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"What TV shows or books help you read people's body language?\\n\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "with open(\"./quora.txt\", encoding=\"utf-8\") as file:\n",
    "    data = list(file)\n",
    "\n",
    "data[50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fd001525",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['what', 'tv', 'shows', 'or', 'books', 'help', 'you', 'read', 'people', \"'\", 's', 'body', 'language', '?']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import WordPunctTokenizer\n",
    "tokenizer = WordPunctTokenizer()\n",
    "\n",
    "print(tokenizer.tokenize(data[50].lower()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "501931fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_tok = [tokenizer.tokenize(row.lower()) for row in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "58bf1fdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"can i get back with my ex even though she is pregnant with another guy ' s baby ?\", 'what are some ways to overcome a fast food addiction ?']\n"
     ]
    }
   ],
   "source": [
    "print([' '.join(row) for row in data_tok[:2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "96dcd655",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "model = Word2Vec(data_tok, \n",
    "                 vector_size=32,      # embedding vector size\n",
    "                 min_count=5,  # consider words that occured at least 5 times\n",
    "                 window=5).wv  # define context as a 5-word window around the target word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fcac21cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-3.6705184 ,  0.15038745,  1.6393185 ,  2.553356  ,  3.0439389 ,\n",
       "        2.7595148 ,  1.4224917 , -4.3146915 ,  2.0184262 ,  1.6314199 ,\n",
       "       -1.2848033 ,  3.319112  ,  4.0094404 ,  1.5808349 ,  2.5065715 ,\n",
       "       -1.584282  , -0.5537907 , -1.0058753 ,  1.0366745 , -0.6948822 ,\n",
       "       -3.1887562 ,  0.16117893, -1.3835508 , -2.2677662 ,  1.2314115 ,\n",
       "       -1.7679586 , -0.96612245, -0.5095072 ,  0.28183788,  0.05451391,\n",
       "       -1.0221356 , -0.8061581 ], dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now you can get word vectors !\n",
    "model.get_vector('anything')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3def7c3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('rice', 0.954008936882019),\n",
       " ('fruit', 0.9394801259040833),\n",
       " ('cheese', 0.9302470684051514),\n",
       " ('butter', 0.9259763956069946),\n",
       " ('beer', 0.9259364008903503),\n",
       " ('wine', 0.9229521751403809),\n",
       " ('sauce', 0.9184145331382751),\n",
       " ('beans', 0.9124134182929993),\n",
       " ('chocolate', 0.9117215275764465),\n",
       " ('orange', 0.9115733504295349)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# or query similar words directly. Go play with it!\n",
    "model.most_similar('bread')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0ad8d059",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "model = api.load('glove-twitter-100')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7938a034",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<user>', '_', 'please', 'apa', 'justin', 'text', 'hari', 'playing', 'once', 'sei']\n"
     ]
    }
   ],
   "source": [
    "words = model.index_to_key[:1000] \n",
    "\n",
    "print(words[::100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a76d8ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each word, compute it's vector with model\n",
    "word_vectors = np.array([model.get_vector(item) for item in words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5824fc71",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# map word vectors onto 2d plane with PCA. Use good old sklearn api (fit, transform)\n",
    "# after that, normalize vectors to make sure they have zero mean and unit variance\n",
    "pca = PCA(n_components=2, svd_solver='full')\n",
    "word_vectors_pca=pca.fit_transform(word_vectors)\n",
    "mean=np.mean(word_vectors_pca,axis=0)\n",
    "std=np.std(word_vectors_pca,axis=0)\n",
    "word_vectors_pca=(word_vectors_pca-mean)/std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "84f07984",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_phrase_embedding(phrase):\n",
    "    \"\"\"\n",
    "    Convert phrase to a vector by aggregating it's word embeddings. See description above.\n",
    "    \"\"\"\n",
    "    # 1. lowercase phrase\n",
    "    # 2. tokenize phrase\n",
    "    # 3. average word vectors for all words in tokenized phrase\n",
    "    # skip words that are not in model's vocabulary\n",
    "    # if all words are missing from vocabulary, return zeros\n",
    "    \n",
    "    vector = np.zeros([model.vector_size], dtype='float32')\n",
    "    phrase=phrase.lower()\n",
    "    phrase=tokenizer.tokenize(phrase)\n",
    "    words=[model.get_vector(item) for item in phrase if item in model.key_to_index]\n",
    "    if len(words):\n",
    "        vector=np.array(words).mean(axis=0)\n",
    "    \n",
    "    # YOUR CODE\n",
    "    \n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dc8f368f",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector = get_phrase_embedding(\"I'm very sure. This never happened to me before...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "204a084b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's only consider ~5k phrases for a first run.\n",
    "chosen_phrases = data[::len(data) // 1000]\n",
    "\n",
    "#print(len(chosen_phrases))\n",
    "# compute vectors for chosen phrases\n",
    "phrase_vectors = [get_phrase_embedding(item) for item in chosen_phrases]\n",
    "phrase_vectors= np.array(phrase_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b78886ff",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'TSNE' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# map vectors into 2d space with pca, tsne or your other method of choice\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# don't forget to normalize\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m phrase_vectors_2d \u001b[38;5;241m=\u001b[39m \u001b[43mTSNE\u001b[49m()\u001b[38;5;241m.\u001b[39mfit_transform(phrase_vectors)\n\u001b[1;32m      6\u001b[0m phrase_vectors_2d \u001b[38;5;241m=\u001b[39m (phrase_vectors_2d \u001b[38;5;241m-\u001b[39m phrase_vectors_2d\u001b[38;5;241m.\u001b[39mmean(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)) \u001b[38;5;241m/\u001b[39m phrase_vectors_2d\u001b[38;5;241m.\u001b[39mstd(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'TSNE' is not defined"
     ]
    }
   ],
   "source": [
    "# map vectors into 2d space with pca, tsne or your other method of choice\n",
    "# don't forget to normalize\n",
    "\n",
    "phrase_vectors_2d = TSNE().fit_transform(phrase_vectors)\n",
    "\n",
    "phrase_vectors_2d = (phrase_vectors_2d - phrase_vectors_2d.mean(axis=0)) / phrase_vectors_2d.std(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5687090c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute vector embedding for all lines in data\n",
    "data_vectors = np.array([get_phrase_embedding(l) for l in data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "00d5d961",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4ce79ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_nearest(query, k=10):\n",
    "    \"\"\"\n",
    "    given text line (query), return k most similar lines from data, sorted from most to least similar\n",
    "    similarity should be measured as cosine between query and line embedding vectors\n",
    "    hint: it's okay to use global variables: data and data_vectors. see also: np.argpartition, np.argsort\n",
    "    \"\"\"\n",
    "    q=get_phrase_embedding(query)\n",
    "    q=q[None,]\n",
    "    similarity=cosine_similarity(data_vectors,q)[...,0]\n",
    "    topk=similarity.argsort()[::-1][:k]\n",
    "    topk_phrase=[data[k] for k in topk]\n",
    "    return topk_phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d3cf344",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = find_nearest(query=\"How do i enter the matrix?\", k=10)\n",
    "\n",
    "print(''.join(results))\n",
    "\n",
    "assert len(results) == 10 and isinstance(results[0], str)\n",
    "assert results[0] == 'How do I get to the dark web?\\n'\n",
    "assert results[3] == 'What can I do to save the world?\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7708f60d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
