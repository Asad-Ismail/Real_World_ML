{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-29T14:56:41.341681Z",
     "iopub.status.busy": "2025-07-29T14:56:41.341229Z",
     "iopub.status.idle": "2025-07-29T14:56:44.981857Z",
     "shell.execute_reply": "2025-07-29T14:56:44.981216Z",
     "shell.execute_reply.started": "2025-07-29T14:56:41.341650Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-29T14:56:44.985839Z",
     "iopub.status.busy": "2025-07-29T14:56:44.985474Z",
     "iopub.status.idle": "2025-07-29T14:56:45.018067Z",
     "shell.execute_reply": "2025-07-29T14:56:45.017488Z",
     "shell.execute_reply.started": "2025-07-29T14:56:44.985807Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()  \n",
    "hf_token = os.getenv(\"HF_TOKEN\") \n",
    "login(token=hf_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-29T14:56:45.019111Z",
     "iopub.status.busy": "2025-07-29T14:56:45.018806Z",
     "iopub.status.idle": "2025-07-29T14:56:45.417349Z",
     "shell.execute_reply": "2025-07-29T14:56:45.416715Z",
     "shell.execute_reply.started": "2025-07-29T14:56:45.019089Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load dataset from Hugging Face Hub\n",
    "dataset_id = \"Jiayi-Pan/Countdown-Tasks-3to4\"\n",
    "dataset = load_dataset(dataset_id, split=\"train\")\n",
    "# select a random subset of 50k samples\n",
    "dataset = dataset.shuffle(seed=42).select(range(50000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-29T14:56:45.419325Z",
     "iopub.status.busy": "2025-07-29T14:56:45.418832Z",
     "iopub.status.idle": "2025-07-29T14:56:45.945092Z",
     "shell.execute_reply": "2025-07-29T14:56:45.944505Z",
     "shell.execute_reply.started": "2025-07-29T14:56:45.419301Z"
    }
   },
   "outputs": [],
   "source": [
    "# also try smaller model Qwen/Qwen3-0.6B, Qwen/Qwen2.5-3B-Instruct\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen3-0.6B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-29T14:56:45.946306Z",
     "iopub.status.busy": "2025-07-29T14:56:45.946012Z",
     "iopub.status.idle": "2025-07-29T14:56:46.090792Z",
     "shell.execute_reply": "2025-07-29T14:56:46.090249Z",
     "shell.execute_reply.started": "2025-07-29T14:56:45.946284Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate_r1_prompt(numbers, target):\n",
    "    r1_prefix = [{\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a helpful assistant. You first thinks about the reasoning process in the mind and then provides the user with the answer.\"\n",
    "      },\n",
    "      { \n",
    "        \"role\": \"user\",\n",
    "        \"content\": f\"Using the numbers {numbers}, create an equation that equals {target}. You can use basic arithmetic operations (+, -, *, /) and each number can only be used once. Show your work in <think> </think> tags. And return the final equation and answer in <answer> </answer> tags, for example <answer> (1 + 2) / 3 = 1 </answer>.\"\n",
    "      },\n",
    "      {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": \"Let me solve this step by step.\\n<think>\"\n",
    "      }]\n",
    "    return {\"prompt\": tokenizer.apply_chat_template(r1_prefix, tokenize=False, continue_final_message=True), \"target\": target}\n",
    " \n",
    "# convert our dataset to the r1 prompt\n",
    "dataset = dataset.map(lambda x: generate_r1_prompt(x[\"nums\"], x[\"target\"]))\n",
    " \n",
    "# split the dataset into train and test\n",
    "train_test_split = dataset.train_test_split(test_size=0.1)\n",
    " \n",
    "train_dataset = train_test_split[\"train\"]\n",
    "test_dataset = train_test_split[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-29T14:56:46.222263Z",
     "iopub.status.busy": "2025-07-29T14:56:46.221925Z",
     "iopub.status.idle": "2025-07-29T14:56:46.226448Z",
     "shell.execute_reply": "2025-07-29T14:56:46.225893Z",
     "shell.execute_reply.started": "2025-07-29T14:56:46.222239Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['target', 'nums', 'prompt'])\n"
     ]
    }
   ],
   "source": [
    "for item in train_dataset:\n",
    "    #print(item)\n",
    "    print(item.keys())\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-29T14:56:47.370903Z",
     "iopub.status.busy": "2025-07-29T14:56:47.370358Z",
     "iopub.status.idle": "2025-07-29T14:56:47.379527Z",
     "shell.execute_reply": "2025-07-29T14:56:47.378855Z",
     "shell.execute_reply.started": "2025-07-29T14:56:47.370877Z"
    }
   },
   "outputs": [],
   "source": [
    "def format_reward_func(completions, target, **kwargs):\n",
    "    \"\"\"\n",
    "    Format: <think>...</think><answer>...</answer>\n",
    "    Args:\n",
    "        completions (list[str]): Generated outputs\n",
    "        target (list[str]): Expected answers\n",
    "      \n",
    "      Returns:\n",
    "          list[float]: Reward scores\n",
    "    \"\"\"\n",
    "    rewards = []\n",
    " \n",
    "    for completion, gt in zip(completions, target):\n",
    " \n",
    "      try:\n",
    "        # add synthetic <think> as its already part of the prompt and prefilled for the assistant to more easily match the regex\n",
    "        completion = \"<think>\" + completion        \n",
    "        # Check if the format is correct\n",
    "        regex = r\"^<think>([^<]*(?:<(?!/?think>)[^<]*)*)<\\/think>\\n<answer>([\\s\\S]*?)<\\/answer>$\"\n",
    " \n",
    "        match = re.search(regex, completion, re.DOTALL) \n",
    "        # if the format is not correct, reward is 0\n",
    "        if match is None or len(match.groups()) != 2:\n",
    "            rewards.append(0.0)\n",
    "        else:\n",
    "            rewards.append(1.0)\n",
    "      except Exception:\n",
    "        rewards.append(0.0)\n",
    "    return rewards\n",
    " \n",
    "def equation_reward_func(completions, target, nums, **kwargs):\n",
    "    \"\"\"\n",
    "    Evaluates completions based on:\n",
    "    2. Mathematical correctness of the answer\n",
    " \n",
    "    Args:\n",
    "        completions (list[str]): Generated outputs\n",
    "        target (list[str]): Expected answers\n",
    "        nums (list[str]): Available numbers\n",
    "    \n",
    "    Returns:\n",
    "        list[float]: Reward scores\n",
    "    \"\"\"\n",
    "    rewards = []\n",
    "    for completion, gt, numbers in zip(completions, target, nums):\n",
    "      try:\n",
    "        # add synthetic <think> as its already part of the prompt and prefilled for the assistant to more easily match the regex\n",
    "        completion = \"<think>\" + completion\n",
    "        # Check if the format is correct\n",
    "        match = re.search(r\"<answer>(.*?)<\\/answer>\", completion)\n",
    "        if match is None:\n",
    "            rewards.append(0.0)\n",
    "            continue\n",
    "        # Extract the \"answer\" part from the completion\n",
    "        equation = match.group(1).strip()\n",
    "        # Extract all numbers from the equation\n",
    "        used_numbers = [int(n) for n in re.findall(r'\\d+', equation)]\n",
    "        \n",
    "        # Check if all numbers are used exactly once\n",
    "        if sorted(used_numbers) != sorted(numbers):\n",
    "            rewards.append(0.0)\n",
    "            continue\n",
    "        # Define a regex pattern that only allows numbers, operators, parentheses, and whitespace\n",
    "        allowed_pattern = r'^[\\d+\\-*/().\\s]+$'\n",
    "        if not re.match(allowed_pattern, equation):\n",
    "           rewards.append(0.0)\n",
    "           continue\n",
    "        \n",
    "        # Evaluate the equation with restricted globals and locals\n",
    "        result = eval(equation, {\"__builtins__\": None}, {})\n",
    "        # Check if the equation is correct and matches the ground truth\n",
    "        if abs(float(result) - float(gt)) < 1e-5:\n",
    "            rewards.append(1.0)\n",
    "        else:\n",
    "            rewards.append(0.0)\n",
    "      except Exception:\n",
    "            # If evaluation fails, reward is 0\n",
    "            rewards.append(0.0) \n",
    "    return rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-29T14:56:48.286209Z",
     "iopub.status.busy": "2025-07-29T14:56:48.285866Z",
     "iopub.status.idle": "2025-07-29T14:56:48.292173Z",
     "shell.execute_reply": "2025-07-29T14:56:48.291478Z",
     "shell.execute_reply.started": "2025-07-29T14:56:48.286184Z"
    }
   },
   "outputs": [],
   "source": [
    "correct_sample_1 = \"\"\"We need to find an equation using the numbers 19, 36, 55, and 7\n",
    "exactly once, with basic arithmetic operations, that equals 65. One possible\n",
    "combination is 55 + 36 - 19 + 7... </think>\n",
    "<answer> 55 + 36 - 7 - 19 </answer>\"\"\"\n",
    " \n",
    "correct_sample_2 = \"\"\" ... </think>\n",
    "<answer> 55 + 36 - 7 - 19 </answer>\"\"\"\n",
    " \n",
    "wrong_format = \"\"\"User: Using the numbers [19, 36, 55, 7], create an equation that equals 65.\"\"\"\n",
    " \n",
    "wrong_format_2 = \"\"\"To find the equation that equals 79 using the numbers 95, 78, 6, 88, I'll start by adding 88 and 95:                      \n",
    "95 + 88 = 183                                                                                                              \n",
    "Now, let's subtract 104 from 183 to get 79:\n",
    "183 - 104 = 79\n",
    "<think> 183 - 104 = 79 </think><think> 183 - 104 = 79 </think><answer> 183 - 104 = 79 </answer>\"\"\"\n",
    " \n",
    "wrong_result = \"\"\" ... </think>\n",
    "<answer> 55 + 36 - 7 - 18 </answer>\"\"\"\n",
    " \n",
    " \n",
    "test_rewards = format_reward_func(completions=[correct_sample_1, correct_sample_2, wrong_format, wrong_format_2, wrong_result], target=[\"65\", \"65\", \"65\", \"65\", \"65\"], nums=[[19, 36, 55, 7]] * 5)\n",
    "assert test_rewards == [1.0, 1.0, 0.0, 0.0, 1.0], \"Reward function is not working\"\n",
    "test_rewards = equation_reward_func(completions=[correct_sample_1, correct_sample_2, wrong_format, wrong_format_2, wrong_result], target=[\"65\", \"65\", \"65\", \"65\", \"65\"], nums=[[19, 36, 55, 7]] * 5)\n",
    "assert test_rewards == [1.0, 1.0, 0.0, 0.0, 0.0], \"Reward function is not working\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-29T15:00:53.830369Z",
     "iopub.status.busy": "2025-07-29T15:00:53.830014Z",
     "iopub.status.idle": "2025-07-29T15:01:45.641553Z",
     "shell.execute_reply": "2025-07-29T15:01:45.640944Z",
     "shell.execute_reply.started": "2025-07-29T15:00:53.830347Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "625742b5e28d43d78314771b29b59063",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from trl import GRPOConfig, GRPOTrainer, get_peft_config, ModelConfig\n",
    " \n",
    "# our model we are going to use as policy \n",
    "model_config = ModelConfig(\n",
    "    model_name_or_path=\"Qwen/Qwen2.5-3B-Instruct\",\n",
    "    torch_dtype=\"bfloat16\",\n",
    "    attn_implementation=\"flash_attention_2\",\n",
    "    use_peft=True,\n",
    "    load_in_4bit=True,\n",
    ")\n",
    " \n",
    "# Hyperparameters\n",
    "training_args = GRPOConfig(\n",
    "    output_dir=\"qwen-r1-aha-moment\",\n",
    "    learning_rate=5e-7,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    logging_steps=10,\n",
    "    max_steps=100,\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=1,\n",
    "    gradient_checkpointing=True,\n",
    "    gradient_checkpointing_kwargs={\"use_reentrant\": False},\n",
    "    fp16=True,\n",
    "    # GRPO specific parameters\n",
    "    max_prompt_length=256,\n",
    "    max_completion_length=1024, # max length of the generated output for our solution\n",
    "    num_generations=2,\n",
    "    beta=0.001,)\n",
    "\n",
    "trainer = GRPOTrainer(\n",
    "    model=model_config.model_name_or_path,\n",
    "    reward_funcs=[format_reward_func, equation_reward_func],\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    peft_config=get_peft_config(model_config),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
