{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Set, Tuple\n",
    "\n",
    "# Third-party imports\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from datasets import Dataset, DatasetDict, load_dataset\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_categories(dataset):\n",
    "    \"\"\"\n",
    "    Analyze clothing categories in a fashion dataset.\n",
    "    \n",
    "    Args:\n",
    "        dataset\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: Statistics for each category\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get category names from features\n",
    "    category_names = dataset['train'].features['objects'].feature['category'].names\n",
    "    \n",
    "    # Initialize statistics\n",
    "    stats = defaultdict(lambda: {\n",
    "        'count': 0,\n",
    "        'total_area': 0,\n",
    "        'areas': []\n",
    "    })\n",
    "    \n",
    "    # Process training set\n",
    "    print(\"Analyzing object statistics...\")\n",
    "    for item in tqdm(dataset['train']):\n",
    "        objects = item['objects']\n",
    "        for cat_id, area in zip(objects['category'], objects['area']):\n",
    "            cat_name = category_names[cat_id]\n",
    "            stats[cat_name]['count'] += 1\n",
    "            stats[cat_name]['total_area'] += area\n",
    "            stats[cat_name]['areas'].append(area)\n",
    "    \n",
    "    # Calculate statistics and create DataFrame\n",
    "    data = []\n",
    "    for cat, metrics in stats.items():\n",
    "        if metrics['count'] > 0:  # Avoid division by zero\n",
    "            data.append({\n",
    "                'category': cat,\n",
    "                'count': metrics['count'],\n",
    "                'avg_area': metrics['total_area'] / metrics['count'],\n",
    "                'area_std': np.std(metrics['areas']),\n",
    "                'is_part': cat.lower() in {'collar', 'pocket', 'sleeve', \n",
    "                                         'zipper', 'button', 'buckle'}\n",
    "            })\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    # Calculate combined score and sort\n",
    "    df['score'] = df['count'] * df['avg_area']\n",
    "    df = df.sort_values('score', ascending=False)\n",
    "    \n",
    "    # Filter and display main garments\n",
    "    main_garments = df[~df['is_part']].head(10)\n",
    "    print(\"\\nTop 10 Main Garments by frequency and size:\")\n",
    "    print(main_garments[['category', 'count', 'avg_area']].round(2).to_string())\n",
    "    \n",
    "    # Visualize\n",
    "    plt.figure(figsize=(15, 6))\n",
    "    plt.bar(main_garments['category'], main_garments['avg_area'])\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.title('Average Area by Category')\n",
    "    plt.ylabel('Average Area (pixelsÂ²)')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43fd292267d14c15b7e02a986ee94711",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading metadata:   0%|          | 0.00/1.78k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68bef02a34814b78b1e64be79114751a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/5.22k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset None/None (download: 3.24 GiB, generated: 3.12 GiB, post-processed: Unknown size, total: 6.36 GiB) to /home/asad/.cache/huggingface/datasets/detection-datasets___parquet/detection-datasets--fashionpedia-fd367b48ab385b58/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "394bd0d997824da1be25c5f58582cc05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "acba8953fba44775a34bebbb3a962996",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/482M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6417504be3df445ca75d48cd30a7d5e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/480M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset_name=\"detection-datasets/fashionpedia\"\n",
    "dataset = load_dataset(dataset_name)\n",
    "print(\"Dataset loaded!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_categories(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_category_images(\n",
    "    dataset_dict,\n",
    "    categories: List[str],\n",
    "    n_train: int = 100,\n",
    "    n_val: int = 20,\n",
    "    seed: int = 42\n",
    ") -> Tuple[Dataset, Dataset]:\n",
    "    \"\"\"\n",
    "    Select images containing ALL specified categories for training and validation.\n",
    "    \n",
    "    Args:\n",
    "        dataset_dict: HuggingFace DatasetDict\n",
    "        categories: List of category names to include\n",
    "        n_train: Number of training images to select\n",
    "        n_val: Number of validation images to select\n",
    "        seed: Random seed for reproducibility\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (training_dataset, validation_dataset)\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # Access the training split\n",
    "    dataset = dataset_dict['train']\n",
    "    \n",
    "    # Get category IDs from names\n",
    "    category_ids = {\n",
    "        name: idx for idx, name in enumerate(\n",
    "            dataset.features['objects'].feature['category'].names\n",
    "        )\n",
    "    }\n",
    "    \n",
    "    # Convert input categories to IDs\n",
    "    target_cat_ids = {category_ids[cat] for cat in categories if cat in category_ids}\n",
    "    \n",
    "    # Find images containing ALL target categories\n",
    "    valid_indices = []\n",
    "    for idx, item in enumerate(dataset):\n",
    "        cats_in_image = set(item['objects']['category'])\n",
    "        # Check if ALL target categories are in this image\n",
    "        if target_cat_ids.issubset(cats_in_image):\n",
    "            valid_indices.append(idx)\n",
    "    \n",
    "    # Ensure we have enough images\n",
    "    total_needed = n_train + n_val\n",
    "    if len(valid_indices) < total_needed:\n",
    "        raise ValueError(\n",
    "            f\"Not enough images with ALL specified categories. \"\n",
    "            f\"Found {len(valid_indices)}, need {total_needed}. \"\n",
    "            f\"Try reducing n_train and n_val or selecting fewer categories.\"\n",
    "        )\n",
    "    \n",
    "    # Shuffle indices\n",
    "    random.shuffle(valid_indices)\n",
    "    \n",
    "    # Split into train and val\n",
    "    train_indices = valid_indices[:n_train]\n",
    "    val_indices = valid_indices[n_train:n_train + n_val]\n",
    "    \n",
    "    # Create new datasets\n",
    "    train_dataset = dataset.select(train_indices)\n",
    "    val_dataset = dataset.select(val_indices)\n",
    "    \n",
    "    # Print statistics\n",
    "    print(\"\\nDataset split statistics:\")\n",
    "    print(f\"Total images with all categories: {len(valid_indices)}\")\n",
    "    print(f\"Training images: {len(train_dataset)}\")\n",
    "    print(f\"Validation images: {len(val_dataset)}\")\n",
    "    \n",
    "    # Print category distribution\n",
    "    print(\"\\nCategory distribution:\")\n",
    "    for split_name, split_data in [(\"Train\", train_dataset), (\"Val\", val_dataset)]:\n",
    "        print(f\"\\n{split_name} split:\")\n",
    "        cat_counts = defaultdict(int)\n",
    "        for item in split_data:\n",
    "            for cat_id in item['objects']['category']:\n",
    "                if cat_id in target_cat_ids:\n",
    "                    cat_name = dataset.features['objects'].feature['category'].names[cat_id]\n",
    "                    cat_counts[cat_name] += 1\n",
    "        \n",
    "        for cat in categories:\n",
    "            print(f\"{cat}: {cat_counts[cat]} instances\")\n",
    "    \n",
    "    return train_dataset, val_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset split statistics:\n",
      "Total images with all categories: 368\n",
      "Training images: 200\n",
      "Validation images: 50\n",
      "\n",
      "Category distribution:\n",
      "\n",
      "Train split:\n",
      "pants: 201 instances\n",
      "bag, wallet: 215 instances\n",
      "shirt, blouse: 206 instances\n",
      "\n",
      "Val split:\n",
      "pants: 50 instances\n",
      "bag, wallet: 53 instances\n",
      "shirt, blouse: 50 instances\n"
     ]
    }
   ],
   "source": [
    "# Trying with fewer categories to find images with all categories\n",
    "categories_of_interest = [\n",
    "    \"pants\",\n",
    "     \"bag, wallet\", \n",
    "    \"shirt, blouse\"  # Starting with just two categories as example\n",
    "]\n",
    "\n",
    "try:\n",
    "    train_ds, val_ds = select_category_images(\n",
    "        dataset,\n",
    "        categories=categories_of_interest,\n",
    "        n_train=200,  # Reduced numbers since we need images with ALL categories\n",
    "        n_val=50\n",
    "    )\n",
    "except ValueError as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    print(\"Try reducing the number of categories or the required images.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_and_save_dataset(\n",
    "    dataset,\n",
    "    categories: List[str],\n",
    "    save_dir: str,\n",
    "    num_images: int = 5,\n",
    "    thickness: int = 2,\n",
    "    font_scale: float = 0.8\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Visualize images with bounding boxes and labels using OpenCV.\n",
    "    \n",
    "    Args:\n",
    "        dataset: HuggingFace dataset split\n",
    "        categories: List of category names to highlight\n",
    "        save_dir: Directory to save visualizations\n",
    "        num_images: Number of images to visualize\n",
    "        thickness: Line thickness for bounding boxes\n",
    "        font_scale: Font scale for labels\n",
    "    \"\"\"\n",
    "    # Create save directory if it doesn't exist\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    # Get category IDs\n",
    "    category_ids = {\n",
    "        name: idx for idx, name in enumerate(\n",
    "            dataset.features['objects'].feature['category'].names\n",
    "        )\n",
    "    }\n",
    "    target_cat_ids = {category_ids[cat] for cat in categories}\n",
    "    \n",
    "    # Create color map for categories (BGR format for OpenCV)\n",
    "    np.random.seed(42)  # for reproducible colors\n",
    "    colors = np.random.randint(0, 255, size=(len(categories), 3)).tolist()\n",
    "    color_map = dict(zip(categories, colors))\n",
    "    \n",
    "    # Font for OpenCV\n",
    "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "    \n",
    "    # Process specified number of images\n",
    "    for i in range(min(num_images, len(dataset))):\n",
    "        try:\n",
    "            example = dataset[i]\n",
    "            \n",
    "            # Convert PIL image to numpy array if necessary\n",
    "            image = np.array(example['image'])\n",
    "            \n",
    "            # Convert to BGR format for OpenCV\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "            \n",
    "            objects = example['objects']\n",
    "            \n",
    "            # Get image dimensions\n",
    "            img_height, img_width = image.shape[:2]\n",
    "            print(f\"Processing image {i+1}, shape: {image.shape}\")\n",
    "            \n",
    "            # Create a copy of the image to draw on\n",
    "            img_draw = image.copy()\n",
    "            \n",
    "            # Draw bounding boxes and labels\n",
    "            for bbox, cat_id in zip(objects['bbox'], objects['category']):\n",
    "                if cat_id in target_cat_ids:\n",
    "                    cat_name = dataset.features['objects'].feature['category'].names[cat_id]\n",
    "                    \n",
    "                    # Get bbox coordinates\n",
    "                    x1, y1, x2, y2 = map(int, bbox)  # Convert to integers for OpenCV\n",
    "                    \n",
    "                    # Verify if bbox is within reasonable bounds\n",
    "                    if (x1 < 0 or y1 < 0 or \n",
    "                        x2 > img_width or \n",
    "                        y2 > img_height or \n",
    "                        x2-x1 <= 0 or y2-y1 <= 0):\n",
    "                        print(f\"Warning: Invalid bbox for {cat_name} in image {i+1}\")\n",
    "                        print(f'Box is {xy},{y1}, {x2}, {y2} for image w {img_width}, {img_height}')\n",
    "                        continue\n",
    "                    \n",
    "                    # Get color for this category\n",
    "                    color = color_map[cat_name]\n",
    "                    \n",
    "                    # Draw rectangle\n",
    "                    cv2.rectangle(img_draw, (x1, y1), (x2, y2), color, thickness)\n",
    "                    \n",
    "                    # Add label with background\n",
    "                    label = cat_name\n",
    "                    (label_w, label_h), _ = cv2.getTextSize(label, font, font_scale, thickness)\n",
    "                    \n",
    "                    # Draw label background\n",
    "                    label_y = max(y1 - label_h - 10, 0)  # Keep label above box but within image\n",
    "                    cv2.rectangle(\n",
    "                        img_draw,\n",
    "                        (x1, label_y),\n",
    "                        (x1 + label_w, label_y + label_h + 10),\n",
    "                        color,\n",
    "                        -1  # Filled rectangle\n",
    "                    )\n",
    "                    \n",
    "                    # Draw label text (white)\n",
    "                    cv2.putText(\n",
    "                        img_draw,\n",
    "                        label,\n",
    "                        (x1, label_y + label_h + 5),\n",
    "                        font,\n",
    "                        font_scale,\n",
    "                        (255, 255, 255),  # White text\n",
    "                        thickness\n",
    "                    )\n",
    "            \n",
    "            # Add title (black text with white background)\n",
    "            title = f'Image {i+1}'\n",
    "            (title_w, title_h), _ = cv2.getTextSize(title, font, 1, thickness)\n",
    "            \n",
    "            # Draw title background\n",
    "            cv2.rectangle(\n",
    "                img_draw,\n",
    "                (10, 10),\n",
    "                (10 + title_w, 10 + title_h + 10),\n",
    "                (255, 255, 255),\n",
    "                -1\n",
    "            )\n",
    "            \n",
    "            # Draw title text\n",
    "            cv2.putText(\n",
    "                img_draw,\n",
    "                title,\n",
    "                (10, 10 + title_h + 5),\n",
    "                font,\n",
    "                1,\n",
    "                (0, 0, 0),  # Black text\n",
    "                thickness\n",
    "            )\n",
    "            \n",
    "            # Save image\n",
    "            save_path = os.path.join(save_dir, f'image_{i+1:04d}.jpg')\n",
    "            cv2.imwrite(save_path, img_draw)\n",
    "            \n",
    "            if (i + 1) % 10 == 0:\n",
    "                print(f\"Processed {i+1} images\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing image {i+1}: {str(e)}\")\n",
    "            import traceback\n",
    "            print(traceback.format_exc())\n",
    "            continue\n",
    "    \n",
    "    print(f\"Completed! Saved {min(num_images, len(dataset))} visualizations to {save_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving validation set visualizations...\n",
      "Processing image 1, shape: (731, 1024, 3)\n",
      "Processing image 2, shape: (1024, 682, 3)\n",
      "Processing image 3, shape: (1024, 768, 3)\n",
      "Processing image 4, shape: (1024, 682, 3)\n",
      "Processing image 5, shape: (1024, 682, 3)\n",
      "Processing image 6, shape: (1024, 743, 3)\n",
      "Processing image 7, shape: (1024, 682, 3)\n",
      "Processing image 8, shape: (1024, 559, 3)\n",
      "Processing image 9, shape: (1024, 1024, 3)\n",
      "Processing image 10, shape: (1024, 502, 3)\n",
      "Processed 10 images\n",
      "Processing image 11, shape: (1024, 806, 3)\n",
      "Processing image 12, shape: (683, 1024, 3)\n",
      "Processing image 13, shape: (1024, 1024, 3)\n",
      "Processing image 14, shape: (1024, 682, 3)\n",
      "Processing image 15, shape: (1024, 682, 3)\n",
      "Processing image 16, shape: (1024, 681, 3)\n",
      "Processing image 17, shape: (1024, 695, 3)\n",
      "Processing image 18, shape: (1024, 681, 3)\n",
      "Processing image 19, shape: (1024, 682, 3)\n",
      "Processing image 20, shape: (683, 1024, 3)\n",
      "Processed 20 images\n",
      "Processing image 21, shape: (1024, 1024, 3)\n",
      "Processing image 22, shape: (1024, 683, 3)\n",
      "Processing image 23, shape: (1024, 765, 3)\n",
      "Processing image 24, shape: (1024, 682, 3)\n",
      "Processing image 25, shape: (1024, 1024, 3)\n",
      "Processing image 26, shape: (1024, 682, 3)\n",
      "Processing image 27, shape: (1024, 647, 3)\n",
      "Processing image 28, shape: (1024, 682, 3)\n",
      "Processing image 29, shape: (1024, 681, 3)\n",
      "Processing image 30, shape: (1024, 682, 3)\n",
      "Processed 30 images\n",
      "Processing image 31, shape: (1024, 682, 3)\n",
      "Processing image 32, shape: (1024, 640, 3)\n",
      "Processing image 33, shape: (1024, 766, 3)\n",
      "Processing image 34, shape: (1024, 682, 3)\n",
      "Processing image 35, shape: (1024, 681, 3)\n",
      "Processing image 36, shape: (1024, 682, 3)\n",
      "Processing image 37, shape: (1024, 683, 3)\n",
      "Processing image 38, shape: (682, 1024, 3)\n",
      "Processing image 39, shape: (1024, 681, 3)\n",
      "Processing image 40, shape: (768, 1024, 3)\n",
      "Processed 40 images\n",
      "Processing image 41, shape: (683, 1024, 3)\n",
      "Processing image 42, shape: (1024, 626, 3)\n",
      "Processing image 43, shape: (1024, 681, 3)\n",
      "Processing image 44, shape: (1024, 682, 3)\n",
      "Processing image 45, shape: (1024, 739, 3)\n",
      "Processing image 46, shape: (1024, 682, 3)\n",
      "Processing image 47, shape: (1024, 681, 3)\n",
      "Processing image 48, shape: (1024, 1024, 3)\n",
      "Processing image 49, shape: (1024, 819, 3)\n",
      "Processing image 50, shape: (682, 1024, 3)\n",
      "Processed 50 images\n",
      "Processing image 51, shape: (1024, 682, 3)\n",
      "Processing image 52, shape: (1024, 683, 3)\n",
      "Processing image 53, shape: (1024, 682, 3)\n",
      "Processing image 54, shape: (1024, 682, 3)\n",
      "Processing image 55, shape: (682, 1024, 3)\n",
      "Processing image 56, shape: (683, 1024, 3)\n",
      "Processing image 57, shape: (1024, 682, 3)\n",
      "Processing image 58, shape: (1024, 1024, 3)\n",
      "Processing image 59, shape: (1024, 682, 3)\n",
      "Processing image 60, shape: (1024, 730, 3)\n",
      "Processed 60 images\n",
      "Processing image 61, shape: (1024, 680, 3)\n",
      "Processing image 62, shape: (1024, 682, 3)\n",
      "Processing image 63, shape: (1024, 1024, 3)\n",
      "Processing image 64, shape: (1024, 682, 3)\n",
      "Processing image 65, shape: (1024, 768, 3)\n",
      "Processing image 66, shape: (1024, 683, 3)\n",
      "Processing image 67, shape: (682, 1024, 3)\n",
      "Processing image 68, shape: (1024, 1024, 3)\n",
      "Processing image 69, shape: (1024, 683, 3)\n",
      "Processing image 70, shape: (753, 1024, 3)\n",
      "Processed 70 images\n",
      "Processing image 71, shape: (1024, 683, 3)\n",
      "Processing image 72, shape: (1024, 682, 3)\n",
      "Processing image 73, shape: (1024, 682, 3)\n",
      "Processing image 74, shape: (1024, 1024, 3)\n",
      "Processing image 75, shape: (1024, 682, 3)\n",
      "Processing image 76, shape: (1024, 682, 3)\n",
      "Processing image 77, shape: (1024, 884, 3)\n",
      "Processing image 78, shape: (1024, 1024, 3)\n",
      "Processing image 79, shape: (1024, 681, 3)\n",
      "Processing image 80, shape: (1024, 682, 3)\n",
      "Processed 80 images\n",
      "Processing image 81, shape: (1024, 681, 3)\n",
      "Processing image 82, shape: (682, 1024, 3)\n",
      "Processing image 83, shape: (1024, 681, 3)\n",
      "Processing image 84, shape: (1024, 682, 3)\n",
      "Processing image 85, shape: (1024, 679, 3)\n",
      "Processing image 86, shape: (1024, 1024, 3)\n",
      "Processing image 87, shape: (1024, 682, 3)\n",
      "Processing image 88, shape: (1024, 683, 3)\n",
      "Processing image 89, shape: (1024, 682, 3)\n",
      "Processing image 90, shape: (1024, 1024, 3)\n",
      "Processed 90 images\n",
      "Processing image 91, shape: (1024, 683, 3)\n",
      "Processing image 92, shape: (1024, 680, 3)\n",
      "Processing image 93, shape: (682, 1024, 3)\n",
      "Processing image 94, shape: (1024, 1024, 3)\n",
      "Processing image 95, shape: (1024, 681, 3)\n",
      "Processing image 96, shape: (1024, 682, 3)\n",
      "Processing image 97, shape: (682, 1024, 3)\n",
      "Processing image 98, shape: (1024, 682, 3)\n",
      "Processing image 99, shape: (1024, 682, 3)\n",
      "Processing image 100, shape: (1024, 696, 3)\n",
      "Processed 100 images\n",
      "Processing image 101, shape: (1024, 682, 3)\n",
      "Processing image 102, shape: (1024, 1024, 3)\n",
      "Processing image 103, shape: (1024, 819, 3)\n",
      "Processing image 104, shape: (1024, 1024, 3)\n",
      "Processing image 105, shape: (1024, 682, 3)\n",
      "Processing image 106, shape: (1024, 682, 3)\n",
      "Processing image 107, shape: (1024, 1024, 3)\n",
      "Processing image 108, shape: (680, 1024, 3)\n",
      "Processing image 109, shape: (1024, 682, 3)\n",
      "Processing image 110, shape: (1024, 1024, 3)\n",
      "Processed 110 images\n",
      "Processing image 111, shape: (1024, 682, 3)\n",
      "Processing image 112, shape: (1024, 1024, 3)\n",
      "Processing image 113, shape: (1024, 682, 3)\n",
      "Processing image 114, shape: (1024, 682, 3)\n",
      "Processing image 115, shape: (1024, 681, 3)\n",
      "Processing image 116, shape: (1024, 682, 3)\n",
      "Processing image 117, shape: (1024, 679, 3)\n",
      "Processing image 118, shape: (1024, 683, 3)\n",
      "Processing image 119, shape: (1024, 683, 3)\n",
      "Processing image 120, shape: (1024, 682, 3)\n",
      "Processed 120 images\n",
      "Processing image 121, shape: (1024, 682, 3)\n",
      "Processing image 122, shape: (1024, 739, 3)\n",
      "Processing image 123, shape: (682, 1024, 3)\n",
      "Processing image 124, shape: (1024, 683, 3)\n",
      "Processing image 125, shape: (682, 1024, 3)\n",
      "Processing image 126, shape: (1024, 697, 3)\n",
      "Processing image 127, shape: (1024, 1024, 3)\n",
      "Processing image 128, shape: (1024, 681, 3)\n",
      "Processing image 129, shape: (1024, 682, 3)\n",
      "Processing image 130, shape: (1024, 682, 3)\n",
      "Processed 130 images\n",
      "Processing image 131, shape: (1024, 1024, 3)\n",
      "Processing image 132, shape: (1024, 682, 3)\n",
      "Processing image 133, shape: (1024, 682, 3)\n",
      "Processing image 134, shape: (1024, 819, 3)\n",
      "Processing image 135, shape: (1024, 681, 3)\n",
      "Processing image 136, shape: (1024, 1024, 3)\n",
      "Processing image 137, shape: (1024, 681, 3)\n",
      "Processing image 138, shape: (1024, 682, 3)\n",
      "Processing image 139, shape: (1024, 682, 3)\n",
      "Processing image 140, shape: (1024, 683, 3)\n",
      "Processed 140 images\n",
      "Processing image 141, shape: (1024, 682, 3)\n",
      "Processing image 142, shape: (1024, 682, 3)\n",
      "Processing image 143, shape: (1024, 1024, 3)\n",
      "Processing image 144, shape: (1024, 683, 3)\n",
      "Processing image 145, shape: (1024, 683, 3)\n",
      "Processing image 146, shape: (1024, 682, 3)\n",
      "Processing image 147, shape: (1024, 681, 3)\n",
      "Processing image 148, shape: (680, 1024, 3)\n",
      "Processing image 149, shape: (1024, 682, 3)\n",
      "Processing image 150, shape: (1024, 682, 3)\n",
      "Processed 150 images\n",
      "Processing image 151, shape: (1024, 1024, 3)\n",
      "Processing image 152, shape: (682, 1024, 3)\n",
      "Processing image 153, shape: (683, 1024, 3)\n",
      "Processing image 154, shape: (1024, 1024, 3)\n",
      "Processing image 155, shape: (1024, 683, 3)\n",
      "Processing image 156, shape: (1024, 575, 3)\n",
      "Processing image 157, shape: (1024, 697, 3)\n",
      "Processing image 158, shape: (1024, 682, 3)\n",
      "Processing image 159, shape: (1024, 683, 3)\n",
      "Processing image 160, shape: (1024, 682, 3)\n",
      "Processed 160 images\n",
      "Processing image 161, shape: (1024, 819, 3)\n",
      "Processing image 162, shape: (1024, 682, 3)\n",
      "Processing image 163, shape: (957, 1024, 3)\n",
      "Processing image 164, shape: (1024, 678, 3)\n",
      "Processing image 165, shape: (1024, 683, 3)\n",
      "Processing image 166, shape: (680, 1024, 3)\n",
      "Processing image 167, shape: (1024, 1024, 3)\n",
      "Processing image 168, shape: (1024, 768, 3)\n",
      "Processing image 169, shape: (1024, 768, 3)\n",
      "Processing image 170, shape: (1024, 655, 3)\n",
      "Processed 170 images\n",
      "Processing image 171, shape: (1024, 768, 3)\n",
      "Processing image 172, shape: (1024, 1020, 3)\n",
      "Processing image 173, shape: (1024, 1024, 3)\n",
      "Processing image 174, shape: (1024, 680, 3)\n",
      "Processing image 175, shape: (1024, 682, 3)\n",
      "Processing image 176, shape: (576, 1024, 3)\n",
      "Processing image 177, shape: (682, 1024, 3)\n",
      "Processing image 178, shape: (1024, 1024, 3)\n",
      "Processing image 179, shape: (1024, 767, 3)\n",
      "Processing image 180, shape: (1024, 1024, 3)\n",
      "Processed 180 images\n",
      "Processing image 181, shape: (1024, 682, 3)\n",
      "Processing image 182, shape: (1024, 665, 3)\n",
      "Processing image 183, shape: (1024, 1024, 3)\n",
      "Processing image 184, shape: (1024, 682, 3)\n",
      "Processing image 185, shape: (1024, 682, 3)\n",
      "Processing image 186, shape: (1024, 682, 3)\n",
      "Processing image 187, shape: (1024, 682, 3)\n",
      "Processing image 188, shape: (1024, 682, 3)\n",
      "Processing image 189, shape: (1024, 682, 3)\n",
      "Processing image 190, shape: (1024, 685, 3)\n",
      "Processed 190 images\n",
      "Processing image 191, shape: (1024, 682, 3)\n",
      "Processing image 192, shape: (1024, 1024, 3)\n",
      "Processing image 193, shape: (682, 1024, 3)\n",
      "Processing image 194, shape: (1024, 682, 3)\n",
      "Processing image 195, shape: (684, 1024, 3)\n",
      "Processing image 196, shape: (1024, 767, 3)\n",
      "Processing image 197, shape: (1024, 768, 3)\n",
      "Processing image 198, shape: (1024, 682, 3)\n",
      "Processing image 199, shape: (1024, 1024, 3)\n",
      "Processing image 200, shape: (1024, 681, 3)\n",
      "Processed 200 images\n",
      "Completed! Saved 200 visualizations to train_visualizations\n"
     ]
    }
   ],
   "source": [
    "# Visualize and save validation images\n",
    "print(\"\\nSaving validation set visualizations...\")\n",
    "visualize_and_save_dataset(\n",
    "    train_ds,\n",
    "    categories_of_interest,\n",
    "    num_images=250,\n",
    "    save_dir='train_visualizations'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'List' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconvert_and_save_dataset\u001b[39m(\n\u001b[1;32m      2\u001b[0m     dataset,\n\u001b[0;32m----> 3\u001b[0m     categories: \u001b[43mList\u001b[49m[\u001b[38;5;28mstr\u001b[39m],\n\u001b[1;32m      4\u001b[0m     save_dir: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m      5\u001b[0m     split_name: \u001b[38;5;28mstr\u001b[39m\n\u001b[1;32m      6\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m      7\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;124;03m    Convert dataset to specified format and save images with annotations.\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;124;03m    \u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;124;03m    Format: label_name bbox_x bbox_y bbox_width bbox_height image_name image_width image_height\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;66;03m# Create save directories\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'List' is not defined"
     ]
    }
   ],
   "source": [
    "def simplify_category_name(cat_name: str) -> str:\n",
    "    \"\"\"\n",
    "    Simplify category names by removing commas and taking first term.\n",
    "    Example: 'bag, wallet' -> 'bag'\n",
    "            'shirt, blouse' -> 'shirt'\n",
    "    \"\"\"\n",
    "    return cat_name.split(',')[0].strip()\n",
    "\n",
    "def convert_and_save_dataset(\n",
    "    dataset,\n",
    "    categories: List[str],\n",
    "    save_dir: str,\n",
    "    split_name: str\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Convert dataset to specified format with simplified category names.\n",
    "    \"\"\"\n",
    "    # Create save directories\n",
    "    save_dir = Path(save_dir)\n",
    "    images_dir = save_dir / 'images' / split_name\n",
    "    images_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Get category IDs with simplified names\n",
    "    category_ids = {\n",
    "        name: idx for idx, name in enumerate(\n",
    "            dataset.features['objects'].feature['category'].names\n",
    "        )\n",
    "    }\n",
    "    # Create mapping between original and simplified names\n",
    "    target_cat_ids = {\n",
    "        category_ids[cat]: simplify_category_name(cat) \n",
    "        for cat in categories if cat in category_ids\n",
    "    }\n",
    "    \n",
    "    # Prepare annotation file\n",
    "    annotations_file = save_dir / f'{split_name}_annotations.txt'\n",
    "    \n",
    "    # Process dataset\n",
    "    print(f\"Processing {split_name} split...\")\n",
    "    with open(annotations_file, 'w') as f:\n",
    "        for idx, example in enumerate(tqdm(dataset)):\n",
    "            image = np.array(example['image'])\n",
    "            img_height, img_width = image.shape[:2]\n",
    "            \n",
    "            # Generate image filename\n",
    "            image_filename = f'{split_name}_{idx:06d}.jpg'\n",
    "            \n",
    "            # Save image\n",
    "            image_path = images_dir / image_filename\n",
    "            image_bgr = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "            cv2.imwrite(str(image_path), image_bgr)\n",
    "            \n",
    "            # Process annotations\n",
    "            for cat_id, bbox in zip(example['objects']['category'], \n",
    "                                  example['objects']['bbox']):\n",
    "                if cat_id in target_cat_ids:\n",
    "                    # Use simplified category name\n",
    "                    simple_cat_name = target_cat_ids[cat_id]\n",
    "                    x, y, w, h = map(int, bbox)\n",
    "                    \n",
    "                    # Write annotation line with simplified category name\n",
    "                    annotation_line = (f'{simple_cat_name} {x} {y} {w} {h} '\n",
    "                                     f'{image_filename} {img_width} {img_height}\\n')\n",
    "                    f.write(annotation_line)\n",
    "\n",
    "    print(f\"Completed processing {split_name} split!\")\n",
    "    print(f\"Images saved to: {images_dir}\")\n",
    "    print(f\"Annotations saved to: {annotations_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Convert and save both splits\n",
    "    save_dir = Path('fashion_dataset')\n",
    "\n",
    "    # Process training split\n",
    "    convert_and_save_dataset(\n",
    "        train_ds,\n",
    "        categories=categories_of_interest,\n",
    "        save_dir=save_dir,\n",
    "        split_name='train'\n",
    "    )\n",
    "except:\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
