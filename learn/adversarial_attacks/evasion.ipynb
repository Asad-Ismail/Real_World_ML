{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the first attacks developed against machine learning models were evasion attacks. This is an attack in which an attacker perturbs an input such that a model produces an incorrect output (for example: this photo of a dog is now classified as a corkscrew), but some useful property of the input is not changed (e.g., it still looks like a dog to a person). Most of these attacks are against image classifiers, however we'll look at evading text classifiers in later labs. The process for evading a model is the same across pretty much all modalities -   \n",
    "\n",
    "- Content filters: the \"useful property\" we're preserving is that the phishing email ends up in the target inbox, while the machine learning system being evaded is a spam filter. \n",
    "- Facial Recognition: You wear a mask that obfuscates your face\n",
    "- Sentiment classifiers: It looks like a positive review to a human, but a scathing condemnation to the model.\n",
    "- Malware detector: The ML model labels a malicious file as benign despite it executing the same malicious functionality.\n",
    "\n",
    "In all cases, the core idea is to find some perturbation which will move the input across a decision boundary, which you can recognize as an \"objective function\". Which specific technique you use will largely depend on the operational constraints of your target, particularly what kind of data you're working with, and what kind of access you have to the model. When gearing up for an attack, you should first figure out the modality of the model (which should be fairly obvious), but strictly speaking, it is not a requirement. Next, you should decide what kind of model access you have, as this will also affect the technique or attacks available to use. \n",
    "\n",
    "This lab is going to feel long, and everything will eventually all mush together in your brain. We're going to run a few different attacks and adjust our assumptions each time to reflect the various types of model access will get. Here's a helpful guide where we define some attacks based on the amount of access required (how much data do you have about model internals) and whether the attack is targeted (are you trying to force the classification to a specific class):\n",
    "\n",
    "| Attack | Access | Targeted | \n",
    "| -- | -- | -- |\n",
    "| Random | Any | No |\n",
    "| Carlini L2 | Gradients (Open box attack) | Both | \n",
    "| SimBA | Proabilities (Partial knowledge attack) | Both | \n",
    "| HopSkipJump | Single label (Closed box attack) | No (bc time) | \n",
    "\n",
    "\n",
    "When you're attacking models, access typically comes in one of three flavors:\n",
    "\n",
    "- **Gradient Access** (or \"Open box\" attack): You have complete access to the model, either by stealing it, or by the victim using a pretrained model that you have identified and found your own copy of on say, HuggingFace. In this case, you can use open-box gradient-based attacks. These attacks use the weights (parameters of the model).\n",
    "- **Scores** (or \"gray-box\" attack): You have API access to the model which provides complete numerical outputs of the model. In this case, you can use methods that estimate the gradients from the scores, `{\"class_0: \"0.89, class_1: 0.06, class_2: 0.049, class_3: 0.01}`. These attacks use \"soft\" labels, or probabilities from the output.\n",
    "- **Labels** (or \"closed-box\" attack): You have API access to the model that only returns the label of the input (or, occasionally, the top 5 or so labels). In this case, you are often forced to use noisier techniques that estimate gradients based on sampling. `[class_0, class_1, class_2, class_3]`. These are \"hard\" labels, and represent the represent the most difficult targets, with `[class_0, class_1]` theoretically being the most difficult. Some algorithms (like HopSkipJump) can use any access.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
