{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4762f62-363e-442b-bba3-5f4f66f74205",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /opt/anaconda3/lib/python3.9/site-packages (4.53.2)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.9/site-packages (from transformers) (3.6.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /opt/anaconda3/lib/python3.9/site-packages (from transformers) (0.33.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/anaconda3/lib/python3.9/site-packages (from transformers) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/lib/python3.9/site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/anaconda3/lib/python3.9/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/anaconda3/lib/python3.9/site-packages (from transformers) (2022.7.9)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/lib/python3.9/site-packages (from transformers) (2.28.1)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /opt/anaconda3/lib/python3.9/site-packages (from transformers) (0.21.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /opt/anaconda3/lib/python3.9/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/anaconda3/lib/python3.9/site-packages (from transformers) (4.64.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/anaconda3/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.7.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/anaconda3/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.3.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /opt/anaconda3/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.5)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /opt/anaconda3/lib/python3.9/site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.9/site-packages (from requests->transformers) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/anaconda3/lib/python3.9/site-packages (from requests->transformers) (1.26.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.9/site-packages (from requests->transformers) (2022.12.7)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fd3a79d",
   "metadata": {},
   "source": [
    "## Using a Pretrained tokenizer from Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f077b51-6c77-45cb-a9d9-1cefeff7d0e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import GPT2Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dea54889-18c0-49f4-88dc-74b96cf48439",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cbcfea0d24c4760a1d2c337738d9926",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "725aa1320f8a4caf845635cd77eed4be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38a7ac43513f49b0a5e2923ceef521fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6369bc35ca342deb1a83587a52cb962",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55a76598f6bd48758bcf384746e17093",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load a pre-trained BPE tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5ce3676e-da68-4b25-91b2-355d5f9a0fa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Text: lower newest widest\n",
      "Token IDs: tensor([[21037, 15530, 46232]])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "text = \"lower newest widest\"\n",
    "encoded_input = tokenizer(text, return_tensors='pt') # pt for PyTorch tensors\n",
    "\n",
    "print(\"Input Text:\", text)\n",
    "print(\"Token IDs:\", encoded_input['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9534f59d-4b0b-4d72-93b7-83c708193dd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['lower', 'Ġnewest', 'Ġwidest']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " tokenizer.convert_ids_to_tokens(encoded_input['input_ids'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2571d17d-5239-4349-9152-b0c750f635c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'lower newest widest'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(encoded_input['input_ids'][0],skip_special_tokens=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "602892cc",
   "metadata": {},
   "source": [
    "## Implementation of Tokenizers from scratch\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "acb29fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "    def encode(self, text: str) -> list[int]:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def decode(self, tokens: list[int]) -> str:\n",
    "        raise NotImplementedError\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d0978c39-95df-4b90-840d-a8ffe7fc35ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class charTokenizer(Tokenizer):\n",
    "    def encode(self, text: str) -> list[int]:\n",
    "        return [ord(c) for c in text]\n",
    "\n",
    "    def decode(self, tokens: list[int]) -> str:\n",
    "        return ''.join(chr(token) for token in tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cc1068b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded: [72, 101, 108, 108, 111, 32, 87, 111, 114, 108, 100, 32, 33]\n",
      "Decoded: Hello World !\n"
     ]
    }
   ],
   "source": [
    "## Test char tokenizer\n",
    "tokenizer = charTokenizer()\n",
    "text = \"Hello World !\"\n",
    "tokens = tokenizer.encode(text)\n",
    "print(\"Encoded:\", tokens)\n",
    "decoded = tokenizer.decode(tokens)\n",
    "print(\"Decoded:\", decoded)\n",
    "assert decoded == text, \"The decoded text donot match\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66eb413a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex as re\n",
    "from collections import Counter\n",
    "\n",
    "class BPETokenizer(Tokenizer):\n",
    "\n",
    "    def __init__(self,special_tokens=['<endoftext>']) -> None:\n",
    "        super().__init__()\n",
    "        #merges: dict[tuple[bytes, bytes]] A list of BPE merges produced from training. Each list item\n",
    "        #is a tuple of bytes (<token1>, <token2>), representing that <token1> was merged with <token2>. The merges should be ordered by order of creation.\n",
    "        self.merges = {}\n",
    "        # vocab maps(1:1 mapping) from integers to bytes index remember bytes argument for item needs to be in [] otherwise it will create 0 byte of length n\n",
    "        self.vocab = {i:bytes([i]) for i in range(256)}\n",
    "        self.special_tokens = special_tokens\n",
    "        for tk in special_tokens:\n",
    "            self.vocab[len(self.vocab)]=tk.encode(\"UTF-8\")\n",
    "    \n",
    "    @staticmethod\n",
    "    def pair_stats(counter:Counter)->dict[[tuple],int]:\n",
    "        pair_counts={}\n",
    "        for k,v in counter.items():\n",
    "            for i in range(len(k)-1):\n",
    "                pair=(k[i],k[i+1])\n",
    "                pair_counts[pair]=pair_counts.get(pair,0)+v\n",
    "        return pair_counts\n",
    "\n",
    "    @staticmethod\n",
    "    def merge(counter: Counter, pair: tuple, index: int) -> dict:\n",
    "        pair_counts={}\n",
    "        for k,v in counter.items():\n",
    "            new_k=[]\n",
    "            i = 0\n",
    "            while i < len(k):\n",
    "                if i<len(k)-1 and (k[i],k[i+1])==pair:\n",
    "                    new_k.append(index)\n",
    "                    i+=2\n",
    "                else:\n",
    "                    new_k.append(k[i])\n",
    "                    i+=1\n",
    "            new_k = tuple(new_k)\n",
    "            pair_counts[new_k] = v        \n",
    "        return pair_counts\n",
    "    \n",
    "\n",
    "    def pretokenization(self, text: str) -> list[bytes]:\n",
    "        \"\"\"Splits text into chunks, respecting special tokens.\"\"\"\n",
    "        PAT = r\"\"\"'(?:[sdmt]|ll|ve|re)| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\"\n",
    "        \n",
    "        if self.special_tokens:\n",
    "            special_pattern = f\"({'|'.join(re.escape(s) for s in self.special_tokens)})\"\n",
    "            chunks = re.split(special_pattern, text)\n",
    "        else:\n",
    "            chunks = [text]\n",
    "\n",
    "        splits = []\n",
    "        for chunk in chunks:\n",
    "            if chunk in self.special_tokens:\n",
    "                splits.append(chunk.encode(\"utf-8\"))\n",
    "            else:\n",
    "                splits.extend(s.encode(\"utf-8\") for s in re.findall(PAT, chunk))\n",
    "        return splits\n",
    "\n",
    "\n",
    "    def train_tokenizer(self, text: str, vocab_size=300) -> None:\n",
    "        assert vocab_size>256, \"Vocanb size should be larger than 256 (number of bytes)\"\n",
    "        # Implement BPE training logic here\n",
    "        num_merges = vocab_size - 256\n",
    "        pretokenized=self.pretokenization(text)\n",
    "        pretoken_ids = [tuple(bs) for bs in pretokenized]\n",
    "        idx_count = Counter(pretoken_ids)\n",
    "        pair_stats =  BPETokenizer.pair_stats(idx_count)\n",
    "        merges=0\n",
    "        while merges<num_merges:\n",
    "            best_pair = max(pair_stats, key= lambda pair: (pair_stats[pair],pair))\n",
    "            if not best_pair:\n",
    "                break\n",
    "            # Merge the best pair\n",
    "            new_token = self.vocab[best_pair[0]] + self.vocab[best_pair[1]]\n",
    "            new_idx= len(self.vocab)\n",
    "            ## this is best pair ids for encoding\n",
    "            self.merges[(best_pair[0], best_pair[1])] = new_idx\n",
    "            ## this is bytes merge token for decoding\n",
    "            self.vocab[new_idx] = new_token\n",
    "            # replace oriiginal pair index with new index\n",
    "            idx_count=BPETokenizer.merge(idx_count, best_pair, new_idx)\n",
    "            pair_stats = BPETokenizer.pair_stats(idx_count)\n",
    "            merges += 1\n",
    "\n",
    "    def encode(self, text: str) -> list[int]:\n",
    "        ## apply first pretokenization\n",
    "        pretokenized=self.pretokenization(text)\n",
    "        pretoken_ids = [tuple(bs) for bs in pretokenized]\n",
    "        ## flatten to single list \n",
    "        flatten_ids=[]\n",
    "        for ids in pretoken_ids:\n",
    "            while len(ids)>1:\n",
    "                pairs=[(ids[i], ids[i+1]) for i in range(len(ids)-1)]\n",
    "                # the pair to merge is the one which has the lowest merge index\n",
    "                pair_to_merge = min(pairs, key= lambda pair: self.merges.get(pair, float('inf')))\n",
    "                if pair_to_merge not in self.merges:\n",
    "                    break\n",
    "                merge_ids=[]\n",
    "                i = 0\n",
    "                while i<len(ids):\n",
    "                    if i<len(ids)-1 and (ids[i],ids[i+1])==pair_to_merge:\n",
    "                        merge_ids.append(self.merges[pair_to_merge])\n",
    "                        i+=2\n",
    "                    else:\n",
    "                        merge_ids.append(ids[i])\n",
    "                        i+=1\n",
    "                ids=merge_ids\n",
    "            # append the final ids to flatten_ids               \n",
    "            flatten_ids.extend(ids)\n",
    "        return flatten_ids\n",
    "\n",
    "    def decode(self, tokens: list[int]) -> str:\n",
    "        # Implement BPE decoding logic here\n",
    "        bs=b\"\".join([self.vocab[i] for i in tokens])\n",
    "        return bs.decode('utf-8', errors='replace')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "b75981eb",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/1m/ps1kwqdn5p1bdj1943362s500000gq/T/ipykernel_26302/1863059989.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mbpe\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBPETokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msplits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbpe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_tokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"low low low low low lower lower widest widest widest newest newest newest newest newest newest\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0msplits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/1m/ps1kwqdn5p1bdj1943362s500000gq/T/ipykernel_26302/3226586744.py\u001b[0m in \u001b[0;36mtrain_tokenizer\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbcounter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0msplit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mchr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m             \u001b[0mbscounter\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbscounter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mpretokenized\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'list'"
     ]
    }
   ],
   "source": [
    "bpe=BPETokenizer()\n",
    "splits = bpe.train_tokenizer(\"low low low low low lower lower widest widest widest newest newest newest newest newest newest\")\n",
    "splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa425dbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "240\n",
      "159\n",
      "140\n",
      "142\n",
      "105\n",
      "115\n"
     ]
    }
   ],
   "source": [
    "text=\"🌎is\"\n",
    "bt=text.encode(\"UTF-8\")\n",
    "\n",
    "[] item in bt:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "db97b61c",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "an integer is required (got type list)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/1m/ps1kwqdn5p1bdj1943362s500000gq/T/ipykernel_26302/1540083107.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mchr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m240\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m159\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m140\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m142\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: an integer is required (got type list)"
     ]
    }
   ],
   "source": [
    "chr([240,159,140,142])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad9dd69",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
