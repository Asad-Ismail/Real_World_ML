{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4762f62-363e-442b-bba3-5f4f66f74205",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /opt/anaconda3/lib/python3.9/site-packages (4.53.2)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.9/site-packages (from transformers) (3.6.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /opt/anaconda3/lib/python3.9/site-packages (from transformers) (0.33.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/anaconda3/lib/python3.9/site-packages (from transformers) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/lib/python3.9/site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/anaconda3/lib/python3.9/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/anaconda3/lib/python3.9/site-packages (from transformers) (2022.7.9)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/lib/python3.9/site-packages (from transformers) (2.28.1)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /opt/anaconda3/lib/python3.9/site-packages (from transformers) (0.21.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /opt/anaconda3/lib/python3.9/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/anaconda3/lib/python3.9/site-packages (from transformers) (4.64.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/anaconda3/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.7.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/anaconda3/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.3.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /opt/anaconda3/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.5)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /opt/anaconda3/lib/python3.9/site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.9/site-packages (from requests->transformers) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/anaconda3/lib/python3.9/site-packages (from requests->transformers) (1.26.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.9/site-packages (from requests->transformers) (2022.12.7)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fd3a79d",
   "metadata": {},
   "source": [
    "## Using a Pretrained tokenizer from Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f077b51-6c77-45cb-a9d9-1cefeff7d0e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import GPT2Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dea54889-18c0-49f4-88dc-74b96cf48439",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cbcfea0d24c4760a1d2c337738d9926",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "725aa1320f8a4caf845635cd77eed4be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38a7ac43513f49b0a5e2923ceef521fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6369bc35ca342deb1a83587a52cb962",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55a76598f6bd48758bcf384746e17093",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load a pre-trained BPE tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5ce3676e-da68-4b25-91b2-355d5f9a0fa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Text: lower newest widest\n",
      "Token IDs: tensor([[21037, 15530, 46232]])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "text = \"lower newest widest\"\n",
    "encoded_input = tokenizer(text, return_tensors='pt') # pt for PyTorch tensors\n",
    "\n",
    "print(\"Input Text:\", text)\n",
    "print(\"Token IDs:\", encoded_input['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9534f59d-4b0b-4d72-93b7-83c708193dd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['lower', 'Ġnewest', 'Ġwidest']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " tokenizer.convert_ids_to_tokens(encoded_input['input_ids'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2571d17d-5239-4349-9152-b0c750f635c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'lower newest widest'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(encoded_input['input_ids'][0],skip_special_tokens=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "602892cc",
   "metadata": {},
   "source": [
    "## Implementation of Tokenizers from scratch\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "acb29fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "    def encode(self, text: str) -> list[int]:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def decode(self, tokens: list[int]) -> str:\n",
    "        raise NotImplementedError\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d0978c39-95df-4b90-840d-a8ffe7fc35ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class charTokenizer(Tokenizer):\n",
    "    def encode(self, text: str) -> list[int]:\n",
    "        return [ord(c) for c in text]\n",
    "\n",
    "    def decode(self, tokens: list[int]) -> str:\n",
    "        return ''.join(chr(token) for token in tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cc1068b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded: [72, 101, 108, 108, 111, 32, 87, 111, 114, 108, 100, 32, 33]\n",
      "Decoded: Hello World !\n"
     ]
    }
   ],
   "source": [
    "## Test char tokenizer\n",
    "tokenizer = charTokenizer()\n",
    "text = \"Hello World !\"\n",
    "tokens = tokenizer.encode(text)\n",
    "print(\"Encoded:\", tokens)\n",
    "decoded = tokenizer.decode(tokens)\n",
    "print(\"Decoded:\", decoded)\n",
    "assert decoded == text, \"The decoded text donot match\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66eb413a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex as re\n",
    "\n",
    "class BPETokenizer(Tokenizer):\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        # BPE merges a list of tuples goes from two bytes to new combined byte\n",
    "        self.merges = []\n",
    "        # vocab maps(1:1 mapping) from integers to bytes index\n",
    "        self.bytes2idx = {i:bytes(i) for i in range(256)}\n",
    "        self.idx2bytes = {v:k for k,v in self.bytes2idx.items()}\n",
    "\n",
    "    @classmethod\n",
    "    def pretokenization(cls,text:str)->list[bytes]:\n",
    "        \"\"\"returns count of bytes to int\"\"\"\n",
    "        PAT = r\"\"\"'(?:[sdmt]|ll|ve|re)| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\"\n",
    "        splits=re.findall(PAT, text)\n",
    "        splits = [s.encode('utf-8') for s in splits]\n",
    "        return splits\n",
    "\n",
    "\n",
    "    def train_tokenizer(self, text: str) -> None:\n",
    "        # Implement BPE training logic here\n",
    "        pass\n",
    "\n",
    "    def encode(self, text: str) -> list[int]:\n",
    "        # Implement BPE encoding logic here\n",
    "        pass\n",
    "\n",
    "    def decode(self, tokens: list[int]) -> str:\n",
    "        # Implement BPE decoding logic here\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b75981eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['somaae', ' text', ' that', ' i', \"'ll\", ' pre', '-', 'tokenasaizea', '!']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bpe=BPETokenizer()\n",
    "splits = bpe.pretokenization(\"somaae text that i'll pre-tokenasaizea!\")\n",
    "splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa425dbe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db97b61c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
