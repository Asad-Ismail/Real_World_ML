{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "zriTdjauH8iQ"
   },
   "outputs": [],
   "source": [
    "#!pip install transformers\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xQiRPWWHlSgv"
   },
   "source": [
    "### Using pre-trained transformers (seminar is worth 2 points)\n",
    "_for fun and profit_\n",
    "\n",
    "There are many toolkits that let you access pre-trained transformer models, but the most powerful and convenient by far is [`huggingface/transformers`](https://github.com/huggingface/transformers). In this week's practice, you'll learn how to download, apply and modify pre-trained transformers for a range of tasks. Buckle up, we're going in!\n",
    "\n",
    "\n",
    "__Pipelines:__ if all you want is to apply a pre-trained model, you can do that in one line of code using pipeline. Huggingface/transformers has a selection of pre-configured pipelines for masked language modelling, sentiment classification, question aswering, etc. ([see full list here](https://huggingface.co/transformers/main_classes/pipelines.html))\n",
    "\n",
    "A typical pipeline includes:\n",
    "* pre-processing, e.g. tokenization, subword segmentation\n",
    "* a backbone model, e.g. bert finetuned for classification\n",
    "* output post-processing\n",
    "\n",
    "Let's see it in action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "rP1KFtvLlJHR"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'POSITIVE', 'score': 0.9998860359191895}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "classifier = pipeline('sentiment-analysis', model=\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "\n",
    "print(classifier(\"BERT is amazing!\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "nYUNuyXMn5l9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Well done!\n"
     ]
    }
   ],
   "source": [
    "import base64\n",
    "data = {\n",
    "    'arryn': 'As High as Honor.',\n",
    "    'baratheon': 'Ours is the fury.',\n",
    "    'stark': 'Winter is coming.',\n",
    "    'tyrell': 'Growing strong.'\n",
    "}\n",
    "\n",
    "# YOUR CODE: predict sentiment for each noble house and create outputs dict\n",
    "outputs = {k:classifier(v)[0][\"label\"]=='POSITIVE' for k,v in data.items()}\n",
    "\n",
    "assert sum(outputs.values()) == 3 and outputs[base64.decodebytes(b'YmFyYXRoZW9u\\n').decode()] == False\n",
    "print(\"Well done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BRDhIH-XpSNo"
   },
   "source": [
    "You can also access vanilla Masked Language Model that was trained to predict masked words. Here's how:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "pa-8noIllRbZ"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P=0.99719 donald trump is the president of the united states.\n",
      "P=0.00024 donald duck is the president of the united states.\n",
      "P=0.00022 donald ross is the president of the united states.\n",
      "P=0.00020 donald johnson is the president of the united states.\n",
      "P=0.00018 donald wilson is the president of the united states.\n"
     ]
    }
   ],
   "source": [
    "mlm_model = pipeline('fill-mask', model=\"bert-base-uncased\")\n",
    "MASK = mlm_model.tokenizer.mask_token\n",
    "\n",
    "for hypo in mlm_model(f\"Donald {MASK} is the president of the united states.\"):\n",
    "  print(f\"P={hypo['score']:.5f}\", hypo['sequence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "9NxeG1Y5pwX1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 0.03938212990760803, 'token': 4585, 'token_str': '1917', 'sequence': 'soviet union was founded in the year 1917.'}\n",
      "{'score': 0.0273779034614563, 'token': 4271, 'token_str': '1918', 'sequence': 'soviet union was founded in the year 1918.'}\n",
      "{'score': 0.015172149054706097, 'token': 4444, 'token_str': '1920', 'sequence': 'soviet union was founded in the year 1920.'}\n",
      "{'score': 0.014308726415038109, 'token': 4878, 'token_str': '1912', 'sequence': 'soviet union was founded in the year 1912.'}\n",
      "{'score': 0.01420625951141119, 'token': 5141, 'token_str': '1900', 'sequence': 'soviet union was founded in the year 1900.'}\n"
     ]
    }
   ],
   "source": [
    "# Your turn: use bert to recall what year was the Soviet Union founded in\n",
    "for hypo in mlm_model(f\"Soviet Union was founded in the year {MASK} .  \"):\n",
    "    print(hypo)\n",
    "  #print(hypo['sequence'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YJxRFzCSq903"
   },
   "source": [
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "Huggingface offers hundreds of pre-trained models that specialize on different tasks. You can quickly find the model you need using [this list](https://huggingface.co/models).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "HRux8Qp2hkXr"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99b2b8cc361944eba428fadbca35c760",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/998 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c42323f0e73474e924ea272c27a92e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/1.33G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "229190ed1ed24d3b8f66a3dce44f32c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/60.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1fff081244e144519f903d63a2c4b4c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "text = \"\"\"Almost two-thirds of the 1.5 million people who viewed this liveblog had Googled to discover\n",
    " the latest on the Rosetta mission. They were treated to this detailed account by the Guardian’s science editor,\n",
    " Ian Sample, and astronomy writer Stuart Clark of the moment scientists landed a robotic spacecraft on a comet \n",
    " for the first time in history, and the delirious reaction it provoked at their headquarters in Germany.\n",
    "  “We are there. We are sitting on the surface. Philae is talking to us,” said one scientist.\n",
    "\"\"\"\n",
    "\n",
    "# Task: create a pipeline for named entity recognition, use task name 'ner' and search for the right model in the list\n",
    "#ner_model = <YOUR CODE>\n",
    "\n",
    "ner_model = pipeline(\"ner\", model=\"dbmdz/bert-large-cased-finetuned-conll03-english\")\n",
    "\n",
    "named_entities = ner_model(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "hf57MRzSiSON"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OUTPUT: [{'entity': 'I-MISC', 'score': 0.88031125, 'index': 19, 'word': 'Google', 'start': 73, 'end': 79}, {'entity': 'I-MISC', 'score': 0.90050775, 'index': 27, 'word': 'Rose', 'start': 112, 'end': 116}, {'entity': 'I-MISC', 'score': 0.95096284, 'index': 28, 'word': '##tta', 'start': 116, 'end': 119}, {'entity': 'I-ORG', 'score': 0.99925345, 'index': 40, 'word': 'Guardian', 'start': 179, 'end': 187}, {'entity': 'I-PER', 'score': 0.999201, 'index': 46, 'word': 'Ian', 'start': 207, 'end': 210}, {'entity': 'I-PER', 'score': 0.9994999, 'index': 47, 'word': 'Sam', 'start': 211, 'end': 214}, {'entity': 'I-PER', 'score': 0.99649787, 'index': 48, 'word': '##ple', 'start': 214, 'end': 217}, {'entity': 'I-PER', 'score': 0.9991856, 'index': 53, 'word': 'Stuart', 'start': 240, 'end': 246}, {'entity': 'I-PER', 'score': 0.99964833, 'index': 54, 'word': 'Clark', 'start': 247, 'end': 252}, {'entity': 'I-LOC', 'score': 0.9998211, 'index': 85, 'word': 'Germany', 'start': 414, 'end': 421}, {'entity': 'I-PER', 'score': 0.6295706, 'index': 99, 'word': 'Phil', 'start': 471, 'end': 475}, {'entity': 'I-PER', 'score': 0.8340385, 'index': 100, 'word': '##ae', 'start': 475, 'end': 477}]\n",
      "All tests passed\n"
     ]
    }
   ],
   "source": [
    "print('OUTPUT:', named_entities)\n",
    "word_to_entity = {item['word']: item['entity'] for item in named_entities}\n",
    "assert 'org' in word_to_entity.get('Guardian').lower() and 'per' in word_to_entity.get('Stuart').lower()\n",
    "print(\"All tests passed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ULMownz6sP9n"
   },
   "source": [
    "### The building blocks of a pipeline\n",
    "\n",
    "Huggingface also allows you to access its pipelines on a lower level. There are two main abstractions for you:\n",
    "* `Tokenizer` - converts from strings to token ids and back\n",
    "* `Model` - a pytorch `nn.Module` with pre-trained weights\n",
    "\n",
    "You can use such models as part of your regular pytorch code: insert is as a layer in your model, apply it to a batch of data, backpropagate, optimize, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "KMJbV0QVsO0Q"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel, pipeline\n",
    "\n",
    "model_name = 'bert-base-uncased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "ZgSPHKPRxG6U"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids tensor([[ 101, 5355, 1010, 1045, 2572, 2115, 2269, 1012,  102,    0,    0,    0,\n",
      "            0,    0,    0],\n",
      "        [ 101, 2166, 2003, 2054, 6433, 2043, 2017, 1005, 2128, 5697, 2437, 2060,\n",
      "         3488, 1012,  102]])\n",
      "input_ids torch.Size([2, 15])\n",
      "token_type_ids tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "token_type_ids torch.Size([2, 15])\n",
      "attention_mask tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
      "attention_mask torch.Size([2, 15])\n",
      "Detokenized:\n",
      "[CLS] luke, i am your father. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "[CLS] life is what happens when you're busy making other plans. [SEP]\n"
     ]
    }
   ],
   "source": [
    "lines = [\n",
    "    \"Luke, I am your father.\",\n",
    "    \"Life is what happens when you're busy making other plans.\",\n",
    "    ]\n",
    "\n",
    "# tokenize a batch of inputs. \"pt\" means [p]y[t]orch tensors\n",
    "tokens_info = tokenizer(lines, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "for key in tokens_info:\n",
    "    print(key, tokens_info[key])\n",
    "    print(key, tokens_info[key].shape)\n",
    "\n",
    "print(\"Detokenized:\")\n",
    "for i in range(2):\n",
    "    print(tokenizer.decode(tokens_info['input_ids'][i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "MJkbHxERyfL4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.8854, -0.4722, -0.9392,  ..., -0.8081, -0.6955,  0.8748],\n",
      "        [-0.9297, -0.5161, -0.9334,  ..., -0.9017, -0.7492,  0.9201]]) torch.Size([2, 768])\n"
     ]
    }
   ],
   "source": [
    "# You can now apply the model to get embeddings\n",
    "with torch.no_grad():\n",
    "    out = model(**tokens_info)\n",
    "\n",
    "print(out['pooler_output'],out['pooler_output'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nucleus_sampling(probs,nucleus=0.9):   \n",
    "    tokens=np.arange(probs.shape[-1])\n",
    "    probs=np.array(probs)\n",
    "    probs_idx=np.argsort(probs)[::-1]\n",
    "    cumsum = np.cumsum(probs[probs_idx])\n",
    "    keep = probs_idx[cumsum <= nucleus]\n",
    "    if not keep.size:\n",
    "        keep = [probs_idx[0]]  # Ensure at least one token is kep\n",
    "    # Set probabilities of non-kept indices to 0\n",
    "    new_probs = np.zeros_like(probs)\n",
    "    new_probs[keep] = probs[keep]\n",
    "    #print(new_probs)\n",
    "    new_probs /= (sum(new_probs))\n",
    "    next_token = np.random.choice(tokens, p=new_probs)\n",
    "    return next_token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OHEC6o7uAfgQ"
   },
   "source": [
    "```\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "__Bonus demo:__ transformer language models. \n",
    "\n",
    "`/* No points awarded for this task, but its really cool, we promise :) */`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "vWCajBGcAern"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The Fermi paradox  is a force that can potentially pull gravity back towards it's zero point\n",
      ". But according to previous experiments  a force that's 100 times greater\n",
      " than gravity is potentially un-stable. Although something like gravity\n",
      " makes large changes that probably violate law,  The Fermi paradox is still\n",
      " a good example of what happens when gravity violates logic. Lifted in\n",
      " the same logic, Weidel's very first experiments prove that the temporal\n",
      " curvature caused by a space-time plastic bubble can be exactly zero and\n",
      " that we can and should do tiny reversals in the models that follow. In\n",
      " addition, a future experiment has found that if Mersenne is certain about\n",
      " zero, there can be zero gravity in the timeline and Gravity will only\n",
      " change slightly if there are zero waves following at the exact moment\n",
      " we perceive it.\n",
      "References\n",
      "2014 The Fermi Paradox explained  physics of\n",
      " the universe, if you can call it that.\n",
      "Lab Experiment   Principles and\n",
      " Practices (2011) - More theory is added as needed to make sure we are\n",
      " a better \"reality\" \n",
      "Psychometry- Kelly  2014 The Fermi Paradox   p. 15\n",
      "<|endoftext|>The Spirit Heights' streaming company Fredericton-based Master\n",
      " Audio announced today that it's beginning its commitment to digitizing\n",
      " content for mobile and home products through the streamster coming soon\n",
      ".\n",
      "\n",
      "Called Master Audio, the streamster is a stable user interface that\n",
      " captures any video you capture within your phone for storage and play\n",
      " within your car or other mobile device. It also includes customisable\n",
      " and personalized apps to enhance your experience in your preferred medium\n",
      ".\n",
      "\n",
      "This experience consists of 128 frames of video (bypasses 120 frames\n",
      " per second or 60fps) in compression between two audio banks. The transmission\n",
      " time varies from about 30 seconds to several minutes depending on the\n",
      " mobile device and monitor (vias 1.3GHz or 1.5GHz).\n",
      "\n",
      "Crown View\n",
      "\n",
      "Durable\n",
      " cables with individually numbered and customisable padding hold the transmission\n",
      " cable down for up to 5 seconds or up to 30 seconds depending on the screen\n",
      " size and original clip length.<|endoftext|>Pleasantly, as the death toll\n",
      " from the Israeli tank attack on Rafah rises in Gaza, and Hamas tries to\n",
      " push further into east Gaza, those responsible for bringing the conflict\n",
      " to a standstill face a difficult question: Should they draw the line?\n",
      "\n",
      "\n",
      "If so, on Tuesday, the head of one of the most prominent Palestinian\n",
      " delegations to the UN, Yasser Arafat, issued a strongly-worded criticism\n",
      " of the resistance, claiming it could be doing the West Bank \"in the wrong\n",
      " way.\" He also claimed that Palestinians were unfairly \"retired\" from occupied\n",
      " territories, but said the Israeli military was at fault for firing on\n",
      " the Rafah buildings, which are vital to the security of the country.\n",
      "\n",
      "\n",
      "These claims were, at best, out of character for the six-month period\n",
      " since the Israeli invasion of Israel. In their absence, Palestinian activists\n",
      " and a local Jewish community have made their case for resistance, and\n",
      " within Israel, the international community has entered into a total war\n",
      " against the Hamas regime, known as the Islamic Jihad, the latter of which\n",
      " came to power in 1948 and has waged war against the Palestinians ever\n",
      " since.\n",
      "\n",
      "But these realities hold important points of contact, one of the\n",
      " major stumbling blocks Palestinians face as they square off in the final\n",
      " battles of their struggle to remain in the West Bank: between the Palestinian\n",
      " Muslim body Islamic Jihad, which rules in the West Bank, and the Palestinian\n",
      " Popular Front, which doesn't reside there and has little chance of political\n",
      " influence in the interim.\n",
      "\n",
      "Those would be the first reports of a Palestinian\n",
      " state splitting in two, seen in Iraq, Syria and Lebanon. These recent\n",
      " events hold forces at odds with each other; Israeli-backed militias based\n",
      " there, including some in the municipal government of Dimona, have drawn\n",
      " fire from the Left side of the political spectrum, but this has been largely\n",
      " unremarkable, there is little evidence of violence by any of these militias\n",
      ", and a minority of security forces within the Palestinian National Council\n",
      ", the political organization that Hamas represents, are explicitly Hamas\n",
      ", but there is no clear indication by the government of Mahmoud Abbas that\n",
      " they are (or ever have been) affiliated with that.\n",
      "\n",
      "It is not clear how\n",
      " the uprising came to be. As was the case with those living inside Gaza\n",
      ", the stakes here remain high.\n",
      "\n",
      "It will be difficult to find much genuine\n",
      " attention to what Hamas means by \"pro-Arabist,\" when only two-thirds of\n",
      " the group's 100,000 members are of Arab or European descent. Yet there\n",
      " is, over the past few months, information suggesting that the insurgency\n",
      " has morphed from a small town of 40,000 people to an army of 60,000. This\n",
      " move is well known by this Islamist-dominated front—who are, as it happens\n",
      ", fighting for the weak Western-backed Gaza Strip. What could be more wrong\n",
      " than a ill-fated series of deadbolts directed at the Islamist movement\n",
      ","
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2', add_prefix_space=True)\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2').train(False).to(device)\n",
    "\n",
    "text = \"The Fermi paradox \"\n",
    "tokens = tokenizer.encode(text)\n",
    "num_steps = 1024 - len(tokens) + 1\n",
    "line_length, max_length = 0, 70\n",
    "\n",
    "print(end=tokenizer.decode(tokens))\n",
    "\n",
    "for i in range(num_steps):\n",
    "    #print(torch.as_tensor([tokens], device=device).shape)\n",
    "    with torch.no_grad():\n",
    "        logits = model(torch.as_tensor([tokens], device=device))[0]\n",
    "    p_next = torch.softmax(logits[0, -1, :], dim=-1).data.cpu().numpy()\n",
    "\n",
    "    #next_token_index = p_next.argmax() #<YOUR CODE: REPLACE THIS LINE>\n",
    "    next_token_index=nucleus_sampling(p_next)\n",
    "    # YOUR TASK: change the code so that it performs nucleus sampling\n",
    "\n",
    "    tokens.append(int(next_token_index))\n",
    "    print(end=tokenizer.decode(tokens[-1]))\n",
    "    line_length += len(tokenizer.decode(tokens[-1]))\n",
    "    if line_length >= max_length:\n",
    "        line_length = 0\n",
    "        print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' The Fermi paradox \\xa0'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_Vij7Gc1wOaq"
   },
   "source": [
    "Transformers knowledge hub: https://huggingface.co/transformers/"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "seminar.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
