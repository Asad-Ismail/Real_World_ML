# AutoEncoder


# AutoEncoder Vs PCA
Autoencoders and principal component analysis (PCA) are both unsupervised learning techniques that can be used for dimensionality reduction. However, while they share some similarities, they are not the same.

PCA is a linear technique that aims to find the directions of maximum variance in the data. It does this by finding the eigenvectors of the covariance matrix of the data, which represent the principal components. PCA can be used for dimensionality reduction by projecting the data onto a lower-dimensional space defined by a subset of the principal components.

Autoencoders, on the other hand, are neural networks that are trained to learn a compressed representation of the input data, called the latent space, by minimizing a reconstruction loss between the input data and the output of the network. The encoder part of the network maps the input data to the latent space, while the decoder part of the network maps the latent space back to the original input space. The idea is that the autoencoder will learn to represent the input data in a compressed form that captures the most important features, while discarding the less important ones.

While autoencoders can be seen as a nonlinear generalization of PCA, they are more powerful than PCA because they can learn more complex, nonlinear relationships in the data. Autoencoders can also be used for a variety of other tasks, such as image generation, data denoising, and anomaly detection.

Using only ~8 % pf parameters we can get the quite good reconstruction of original image for mnist data