## Quitesential Implementation of RL algorithms from scratch

We will learn and implement these concepts/methods

1. Fundamental Concepts:

Markov Decision Processes (MDPs)
Value functions (state-value and action-value functions)
Bellman equations
Exploration vs. Exploitation trade-off

2. Dynamic Programming (DP) methods:
- Policy iteration
- Value iteration

3. Model-free methods:
    a. Temporal-Difference (TD) Learning:
        - SARSA (on-policy)
        - Q-learning (off-policy)

    b. Policy Gradient methods:
        -REINFORCE
        -Actor-Critic algorithms:
        -Advantage Actor-Critic (A2C)
        -Asynchronous Advantage Actor-Critic (A3C)
        -Soft Actor-Critic (SAC)
    c. Deep Reinforcement Learning:
        -Deep Q-Networks (DQN)
        -Double Deep Q-Networks (DDQN)
        -Dueling Deep Q-Networks (Dueling DQN)
        -Proximal Policy Optimization (PPO)

4. Model-based methods:

-Monte Carlo Tree Search (MCTS)
-Dyna-Q
-Model-based planning with learned models

5.Inverse Reinforcement Learning (IRL):

Maximum Entropy IRL
Apprenticeship Learning
Bayesian IRL
Generative Adversarial Imitation Learning (GAIL)


Multi-Agent Reinforcement Learning:

    Independent Q-Learning (IQL)
    Coordinated Reinforcement Learning
    Centralized Training with Decentralized Execution (CTDE)
    Multi-Agent Deep Deterministic Policy Gradient (MADDPG)

Exploration Techniques:

    Epsilon-greedy exploration
    Boltzmann exploration (Softmax)
    Upper Confidence Bound (UCB)
    Thompson Sampling
    Intrinsic motivation (Curiosity-driven exploration)

Hierarchical Reinforcement Learning:
    Options framework
    Hierarchical Abstract Machines (HAMs)
    MAXQ framework
    Feudal Networks

Transfer Learning and Domain Adaptation:

    Progressive Networks
    Distillation methods
    Meta-learning techniques