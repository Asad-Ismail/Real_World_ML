{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/asad/miniconda3/envs/rl310/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from umap import UMAP\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import adjusted_rand_score, normalized_mutual_info_score, silhouette_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting seaborn\n",
      "  Downloading seaborn-0.13.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Requirement already satisfied: numpy!=1.24.0,>=1.20 in /home/asad/miniconda3/envs/rl310/lib/python3.10/site-packages (from seaborn) (2.2.6)\n",
      "Requirement already satisfied: pandas>=1.2 in /home/asad/miniconda3/envs/rl310/lib/python3.10/site-packages (from seaborn) (2.3.1)\n",
      "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in /home/asad/miniconda3/envs/rl310/lib/python3.10/site-packages (from seaborn) (3.10.5)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/asad/miniconda3/envs/rl310/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/asad/miniconda3/envs/rl310/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/asad/miniconda3/envs/rl310/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (4.59.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/asad/miniconda3/envs/rl310/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.4.9)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/asad/miniconda3/envs/rl310/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in /home/asad/miniconda3/envs/rl310/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (11.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/asad/miniconda3/envs/rl310/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (3.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/asad/miniconda3/envs/rl310/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/asad/miniconda3/envs/rl310/lib/python3.10/site-packages (from pandas>=1.2->seaborn) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/asad/miniconda3/envs/rl310/lib/python3.10/site-packages (from pandas>=1.2->seaborn) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/asad/miniconda3/envs/rl310/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.4->seaborn) (1.17.0)\n",
      "Downloading seaborn-0.13.2-py3-none-any.whl (294 kB)\n",
      "Installing collected packages: seaborn\n",
      "Successfully installed seaborn-0.13.2\n"
     ]
    }
   ],
   "source": [
    "#! pip install scikit-learn\n",
    "#! pip install umap-learn\n",
    "!pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path= \"../data/data_sensors.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df= pd.read_csv(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Sensor 0',\n",
       " 'Sensor 1',\n",
       " 'Sensor 2',\n",
       " 'Sensor 3',\n",
       " 'Sensor 4',\n",
       " 'Sensor 5',\n",
       " 'Sensor 6',\n",
       " 'Sensor 7',\n",
       " 'Sensor 8',\n",
       " 'Sensor 9',\n",
       " 'Sensor 10',\n",
       " 'Sensor 11',\n",
       " 'Sensor 12',\n",
       " 'Sensor 13',\n",
       " 'Sensor 14',\n",
       " 'Sensor 15',\n",
       " 'Sensor 16',\n",
       " 'Sensor 17',\n",
       " 'Sensor 18',\n",
       " 'Sensor 19',\n",
       " 'Label']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Shape: (1600, 21)\n",
      "\n",
      "Data Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1600 entries, 0 to 1599\n",
      "Data columns (total 21 columns):\n",
      " #   Column     Non-Null Count  Dtype  \n",
      "---  ------     --------------  -----  \n",
      " 0   Sensor 0   1600 non-null   float64\n",
      " 1   Sensor 1   1600 non-null   float64\n",
      " 2   Sensor 2   1600 non-null   float64\n",
      " 3   Sensor 3   1600 non-null   float64\n",
      " 4   Sensor 4   1600 non-null   float64\n",
      " 5   Sensor 5   1600 non-null   float64\n",
      " 6   Sensor 6   1600 non-null   float64\n",
      " 7   Sensor 7   1600 non-null   float64\n",
      " 8   Sensor 8   1600 non-null   float64\n",
      " 9   Sensor 9   1600 non-null   float64\n",
      " 10  Sensor 10  1600 non-null   float64\n",
      " 11  Sensor 11  1600 non-null   float64\n",
      " 12  Sensor 12  1600 non-null   float64\n",
      " 13  Sensor 13  1600 non-null   float64\n",
      " 14  Sensor 14  1600 non-null   float64\n",
      " 15  Sensor 15  1600 non-null   float64\n",
      " 16  Sensor 16  1600 non-null   float64\n",
      " 17  Sensor 17  1600 non-null   float64\n",
      " 18  Sensor 18  1600 non-null   float64\n",
      " 19  Sensor 19  1600 non-null   float64\n",
      " 20  Label      40 non-null     float64\n",
      "dtypes: float64(21)\n",
      "memory usage: 262.6 KB\n",
      "\n",
      "Missing Values per Column:\n",
      "Sensor 0        0\n",
      "Sensor 1        0\n",
      "Sensor 2        0\n",
      "Sensor 3        0\n",
      "Sensor 4        0\n",
      "Sensor 5        0\n",
      "Sensor 6        0\n",
      "Sensor 7        0\n",
      "Sensor 8        0\n",
      "Sensor 9        0\n",
      "Sensor 10       0\n",
      "Sensor 11       0\n",
      "Sensor 12       0\n",
      "Sensor 13       0\n",
      "Sensor 14       0\n",
      "Sensor 15       0\n",
      "Sensor 16       0\n",
      "Sensor 17       0\n",
      "Sensor 18       0\n",
      "Sensor 19       0\n",
      "Label        1560\n",
      "dtype: int64\n",
      "\n",
      "Total samples: 1600\n",
      "Labeled samples: 40 (2.5%)\n",
      "Unlabeled samples: 1560 (97.5%)\n",
      "\n",
      "Number of unique breakdown types (clusters): k = 3\n",
      "Unique Labels: [np.float64(1.0), np.float64(2.0), np.float64(3.0)]\n"
     ]
    }
   ],
   "source": [
    "print(\"Dataset Shape:\", df.shape)\n",
    "print(\"\\nData Info:\")\n",
    "df.info()\n",
    "print(\"\\nMissing Values per Column:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Separate features and labels\n",
    "\n",
    "features =[x for x in df.columns.tolist() if x!=\"Label\"]\n",
    "X = X = df.drop(columns=[\"Label\"])\n",
    "y = df['Label']\n",
    "\n",
    "# Analyze the labeled vs. unlabeled data\n",
    "labeled_mask = y.notna()\n",
    "unlabeled_mask = y.isna()\n",
    "\n",
    "num_labeled = labeled_mask.sum()\n",
    "num_unlabeled = unlabeled_mask.sum()\n",
    "total_samples = len(df)\n",
    "\n",
    "print(f\"\\nTotal samples: {total_samples}\")\n",
    "print(f\"Labeled samples: {num_labeled} ({num_labeled/total_samples:.1%})\")\n",
    "print(f\"Unlabeled samples: {num_unlabeled} ({num_unlabeled/total_samples:.1%})\")\n",
    "\n",
    "# Determine the number of clusters (k) from the unique labels\n",
    "unique_labels = y.dropna().unique()\n",
    "k = len(unique_labels)\n",
    "print(f\"\\nNumber of unique breakdown types (clusters): k = {k}\")\n",
    "print(\"Unique Labels:\", sorted(unique_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset contains 1600 rows (breakdown events) and 21 columns (20 sensors + 1 label column). Â  \n",
    "There are no missing values in the sensor feature columns.\n",
    "We have a small subset of labeled data: 40 samples are labeled, representing 2.5% of the dataset. The remaining 1560 samples are unlabeled.\n",
    "The labeled data reveals 4 distinct breakdown categories, which sets our number of clusters, k, to 4.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def cluster_vis(X, Y, method=\"PCA\"):\n",
    "    \"\"\"\n",
    "    Visualize high-dimensional data with dimensionality reduction and overlay Y values.\n",
    "    \n",
    "    Parameters:\n",
    "    X : DataFrame, shape (n_samples, n_features)\n",
    "        Input features\n",
    "    Y : DataFrame or Series, shape (n_samples, 1) or (n_samples,)\n",
    "        Target values (mostly NaN with some 1,2,3)\n",
    "    method : str, optional (default=\"PCA\")\n",
    "        Dimensionality reduction method: \"PCA\", \"TSNE\", or \"UMAP\"\n",
    "    \"\"\"\n",
    "    # Validate inputs\n",
    "    if method not in [\"PCA\", \"TSNE\", \"UMAP\"]:\n",
    "        raise ValueError(f\"Method '{method}' not implemented. Available: PCA, TSNE, UMAP\")\n",
    "    \n",
    "    # Perform dimensionality reduction\n",
    "    if method == \"PCA\":\n",
    "        reducer = PCA(n_components=2, random_state=42)\n",
    "        X_transformed = reducer.fit_transform(X)\n",
    "    elif method == \"TSNE\":\n",
    "        reducer = TSNE(n_components=2, random_state=42, perplexity=min(30, X.shape[0]-1))\n",
    "        X_transformed = reducer.fit_transform(X)\n",
    "    elif method == \"UMAP\":\n",
    "        reducer = UMAP(n_components=2, random_state=42)\n",
    "        X_transformed = reducer.fit_transform(X)\n",
    "    \n",
    "    # Create DataFrame with transformed data\n",
    "    df = pd.DataFrame(X_transformed, columns=[f'{method}_1', f'{method}_2'])\n",
    "    \n",
    "    y_values = Y.values \n",
    "    \n",
    "    non_nan_mask = ~np.isnan(y_values)\n",
    "    non_nan_indices = np.where(non_nan_mask)[0]\n",
    "    unique_values = np.unique(y_values[non_nan_mask])\n",
    "    \n",
    "    # Create visualization\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    # Plot all points in gray (background)\n",
    "    plt.scatter(df[f'{method}_1'], df[f'{method}_2'], c='black', alpha=0.7, label='All data')\n",
    "    \n",
    "    # Overlay points with Y values\n",
    "    colors = ['red', 'green', 'blue']\n",
    "    markers = ['o', 's', '^']  # Different markers for each class\n",
    "    \n",
    "    for i, val in enumerate(unique_values):\n",
    "        val_indices = non_nan_indices[y_values[non_nan_indices] == val]\n",
    "        plt.scatter(\n",
    "            df.iloc[val_indices][f'{method}_1'], \n",
    "            df.iloc[val_indices][f'{method}_2'],\n",
    "            c=colors[i % len(colors)],\n",
    "            marker=markers[i % len(markers)],\n",
    "            s=100,\n",
    "            label=f'Class {val}',\n",
    "            edgecolors='black'\n",
    "        )\n",
    "    \n",
    "    # Add plot elements\n",
    "    plt.title(f'{method} Visualization with Y Values Overlay')\n",
    "    plt.xlabel(f'{method} Component 1')\n",
    "    plt.ylabel(f'{method} Component 2')\n",
    "    \n",
    "    # Add explained variance for PCA only\n",
    "    if method == \"PCA\":\n",
    "        plt.xlabel(f'{method} Component 1 (Explains {reducer.explained_variance_ratio_[0]*100:.1f}%)')\n",
    "        plt.ylabel(f'{method} Component 2 (Explains {reducer.explained_variance_ratio_[1]*100:.1f}%)')\n",
    "    \n",
    "    plt.legend()\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"/home/asad/sensor.png\")\n",
    "    plt.show()\n",
    "    \n",
    "    # Print summary statistics\n",
    "    if method == \"PCA\":\n",
    "        print(\"\\nPCA Explained Variance Ratio:\", reducer.explained_variance_ratio_)\n",
    "        print(\"Total Variance Explained by 2 PCs: {:.1f}%\".format(sum(reducer.explained_variance_ratio_)*100))\n",
    "    else:\n",
    "        print(f\"\\n{method} does not provide explained variance metrics\")\n",
    "    \n",
    "    print(\"\\nY Value Distribution:\")\n",
    "    print(pd.Series(y_values[non_nan_mask]).value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Y' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m cluster_vis(X, \u001b[43mY\u001b[49m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUMAP\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Y' is not defined"
     ]
    }
   ],
   "source": [
    "cluster_vis(X, Y, method=\"UMAP\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Seeded K-Means algorithm. The core of this semi-supervised approach is to use the expert-labeled data to create intelligent initial starting points (centroids)\n",
    "# for the K-Means algorithm, guiding it toward a more meaningful solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def cluster_vis(X, Y, method=\"PCA\"):\n",
    "    \"\"\"\n",
    "    Visualize high-dimensional data with dimensionality reduction and overlay Y values.\n",
    "    \n",
    "    Parameters:\n",
    "    X : DataFrame, shape (n_samples, n_features)\n",
    "        Input features\n",
    "    Y : DataFrame or Series, shape (n_samples, 1) or (n_samples,)\n",
    "        Target values (mostly NaN with some 1,2,3)\n",
    "    method : str, optional (default=\"PCA\")\n",
    "        Dimensionality reduction method: \"PCA\", \"TSNE\", or \"UMAP\"\n",
    "    \"\"\"\n",
    "    # Validate inputs\n",
    "    if method not in [\"PCA\", \"TSNE\", \"UMAP\"]:\n",
    "        raise ValueError(f\"Method '{method}' not implemented. Available: PCA, TSNE, UMAP\")\n",
    "    \n",
    "    # Perform dimensionality reduction\n",
    "    if method == \"PCA\":\n",
    "        reducer = PCA(n_components=2, random_state=42)\n",
    "        X_transformed = reducer.fit_transform(X)\n",
    "    elif method == \"TSNE\":\n",
    "        reducer = TSNE(n_components=2, random_state=42, perplexity=min(30, X.shape[0]-1))\n",
    "        X_transformed = reducer.fit_transform(X)\n",
    "    elif method == \"UMAP\":\n",
    "        reducer = UMAP(n_components=2, random_state=42)\n",
    "        X_transformed = reducer.fit_transform(X)\n",
    "    \n",
    "    # Create DataFrame with transformed data\n",
    "    df = pd.DataFrame(X_transformed, columns=[f'{method}_1', f'{method}_2'])\n",
    "    \n",
    "    y_values = Y.values \n",
    "    \n",
    "    non_nan_mask = ~np.isnan(y_values)\n",
    "    non_nan_indices = np.where(non_nan_mask)[0]\n",
    "    unique_values = np.unique(y_values[non_nan_mask])\n",
    "    \n",
    "    # Create visualization\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    # Plot all points in gray (background)\n",
    "    plt.scatter(df[f'{method}_1'], df[f'{method}_2'], c='black', alpha=0.7, label='All data')\n",
    "    \n",
    "    # Overlay points with Y values\n",
    "    colors = ['red', 'green', 'blue']\n",
    "    markers = ['o', 's', '^']  # Different markers for each class\n",
    "    \n",
    "    for i, val in enumerate(unique_values):\n",
    "        val_indices = non_nan_indices[y_values[non_nan_indices] == val]\n",
    "        plt.scatter(\n",
    "            df.iloc[val_indices][f'{method}_1'], \n",
    "            df.iloc[val_indices][f'{method}_2'],\n",
    "            c=colors[i % len(colors)],\n",
    "            marker=markers[i % len(markers)],\n",
    "            s=100,\n",
    "            label=f'Class {val}',\n",
    "            edgecolors='black'\n",
    "        )\n",
    "    \n",
    "    # Add plot elements\n",
    "    plt.title(f'{method} Visualization with Y Values Overlay')\n",
    "    plt.xlabel(f'{method} Component 1')\n",
    "    plt.ylabel(f'{method} Component 2')\n",
    "    \n",
    "    # Add explained variance for PCA only\n",
    "    if method == \"PCA\":\n",
    "        plt.xlabel(f'{method} Component 1 (Explains {reducer.explained_variance_ratio_[0]*100:.1f}%)')\n",
    "        plt.ylabel(f'{method} Component 2 (Explains {reducer.explained_variance_ratio_[1]*100:.1f}%)')\n",
    "    \n",
    "    plt.legend()\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"/home/asad/sensor.png\")\n",
    "    plt.show()\n",
    "    \n",
    "    if method == \"PCA\":\n",
    "        print(\"\\nPCA Explained Variance Ratio:\", reducer.explained_variance_ratio_)\n",
    "        print(\"Total Variance Explained by 2 PCs: {:.1f}%\".format(sum(reducer.explained_variance_ratio_)*100))\n",
    "    else:\n",
    "        print(f\"\\n{method} does not provide explained variance metrics\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of initial centroids: (3, 20)\n"
     ]
    }
   ],
   "source": [
    "# Combine scaled features and original labels for centroid calculation\n",
    "df_scaled = pd.DataFrame(X_scaled, columns=features)\n",
    "df_scaled['Label'] = y\n",
    "\n",
    "# Isolate the labeled data\n",
    "labeled_data_scaled = df_scaled[labeled_mask]\n",
    "\n",
    "# Calculate initial centroids from the labeled data\n",
    "initial_centroids = labeled_data_scaled.groupby('Label')[features].mean().values\n",
    "print(\"Shape of initial centroids:\", initial_centroids.shape)\n",
    "\n",
    "# Configure and train the K-Means model\n",
    "kmeans = KMeans(\n",
    "    n_clusters=k,\n",
    "    init=initial_centroids,\n",
    "    n_init=1,  # We provide the seeds, so only one initialization is needed\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Fit the model on the entire scaled dataset\n",
    "kmeans.fit(X_scaled)\n",
    "\n",
    "# Get the predicted cluster labels for all data points\n",
    "predicted_labels = kmeans.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusted Rand Index (ARI) on labeled data: 0.0978\n",
      "Normalized Mutual Information (NMI) on labeled data: 0.1865\n",
      "Silhouette Score on all data: 0.0363\n"
     ]
    }
   ],
   "source": [
    "# Extract true and predicted labels for the labeled subset\n",
    "true_labels_subset = labeled_data_scaled['Label'].astype(int)\n",
    "predicted_labels_subset = predicted_labels[labeled_mask]\n",
    "\n",
    "# Calculate supervised metrics\n",
    "ari_score = adjusted_rand_score(true_labels_subset, predicted_labels_subset)\n",
    "nmi_score = normalized_mutual_info_score(true_labels_subset, predicted_labels_subset)\n",
    "\n",
    "# Calculate unsupervised metric on the full dataset\n",
    "silhouette = silhouette_score(X_scaled, predicted_labels)\n",
    "\n",
    "print(f\"Adjusted Rand Index (ARI) on labeled data: {ari_score:.4f}\")\n",
    "print(f\"Normalized Mutual Information (NMI) on labeled data: {nmi_score:.4f}\")\n",
    "print(f\"Silhouette Score on all data: {silhouette:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " #results strongly suggest that the underlying clusters in the sensor data are not spherical. They are likely elongated, serpentine, or have other complex, non-convex shapes."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
