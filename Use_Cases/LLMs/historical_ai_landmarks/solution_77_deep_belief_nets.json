{
    "solution_name": "Deep Belief Nets (A Fast Learning Algorithm for Deep Belief Nets)",
    "simplified_problem": "Greedy layer-wise pretraining for deep networks",
    "problem_it_solved": "Training deep neural networks with many layers was extremely difficult due to the vanishing gradient problem, where gradients became exponentially small as they propagated backward through many layers. This made it nearly impossible to effectively train networks with more than 2-3 hidden layers using standard backpropagation, as early layers received almost no useful gradient signal.",
    "historical_context": "In the mid-2000s, deep learning was struggling. Despite theoretical understanding that deeper networks could represent more complex functions, practical training was failing. The dominant approaches were shallow networks (1-2 hidden layers) or carefully hand-crafted features. Researchers had tried various initialization schemes and activation functions, but the fundamental problem of vanishing gradients persisted. Support Vector Machines and other kernel methods were outperforming neural networks on most tasks. The field needed a breakthrough to make deep networks trainable.",
    "landmark_solution_details": {
        "domain": "Deep Learning Optimization",
        "title": "Deep Belief Nets: A Fast Learning Algorithm for Deep Belief Nets",
        "concept": "Instead of training all layers simultaneously with backpropagation, train the network greedily layer-by-layer using Restricted Boltzmann Machines (RBMs). Each layer is trained as an RBM that learns to model the distribution of its input (which is the output of the previous layer). After pretraining, the entire network can be fine-tuned with supervised learning. This approach initializes the weights to good starting points, avoiding poor local minima.",
        "math_foundation": "Each RBM layer is trained using Contrastive Divergence (CD-k), which approximates the log-likelihood gradient. For an RBM with visible units v and hidden units h, the energy function is E(v,h) = -a^T v - b^T h - v^T Wh. The probability distribution is P(v,h) = exp(-E(v,h))/Z. CD-k approximates the gradient by running k steps of Gibbs sampling.",
        "implementation": "1. Train the first layer as an RBM on the raw input data. 2. Use the hidden activations of the first RBM as input to train the second layer as another RBM. 3. Continue this process for all layers. 4. Add a final classification layer on top. 5. Fine-tune the entire network with backpropagation using labeled data.",
        "verification": "Deep Belief Networks achieved state-of-the-art results on MNIST (1.25% error rate) and significantly outperformed shallow networks on various tasks. The paper demonstrated successful training of networks with 5-7 hidden layers, something previously impossible with standard backpropagation.",
        "inspiration": "Statistical Physics (Boltzmann distributions) and the idea of unsupervised pretraining as a form of regularization."
    }
}