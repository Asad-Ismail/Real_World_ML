{
    "solution_name": "MuZero (Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model)",
    "simplified_problem": "Model-based reinforcement learning without knowing the environment dynamics.",
    "problem_it_solved": "How can an agent learn to plan and make decisions in complex environments when it has no prior knowledge of the environment's rules, transition dynamics, or reward structure? Traditional model-based RL requires learning an explicit model of the environment, but this is impossible when the true dynamics are unknown or too complex to model directly.",
    "historical_context": "Before MuZero, the state-of-the-art in game-playing AI was split between two approaches. Model-free methods like DQN and Rainbow could achieve superhuman performance on Atari games but required millions of interactions and couldn't plan ahead. Model-based methods like AlphaZero could master board games through planning, but required perfect knowledge of the game rules and dynamics. This created a fundamental limitation: agents either needed to know the environment's dynamics beforehand (model-based) or learn purely through trial-and-error without planning (model-free). There was no method that could both learn the environment dynamics from scratch AND use planning to improve decision-making.",
    "landmark_solution_details": {
        "domain": "Reinforcement Learning",
        "title": "MuZero: Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model",
        "concept": "MuZero learns to plan by building its own internal model of the environment dynamics, without ever being told the rules. It learns three components: (1) a representation function that maps observations to hidden states, (2) a dynamics function that predicts the next hidden state given an action, and (3) a prediction function that outputs policy and value from any hidden state. These components are trained end-to-end using only the agent's experience, allowing it to plan effectively in unknown environments.",
        "math_foundation": "The learned model consists of three functions: h(s) \u2192 s^0 (representation), g(s^k, a^k) \u2192 s^{k+1}, r^{k+1} (dynamics), and f(s^k) \u2192 \u03c0^k, v^k (prediction). The model is trained to minimize: L = \u03a3_k (r^{k+1} - u^{k+1})^2 + \u03a3_k (v^k - z)^2 - \u03a3_k \u03c0^k log p^k + c||\u03b8||^2, where u^{k+1} are observed rewards, z are observed returns, p^k are MCTS policies, and c is an L2 regularization coefficient.",
        "implementation": "MuZero uses Monte Carlo Tree Search (MCTS) with its learned model for planning. At each step: (1) The representation network encodes the current observation into a hidden state. (2) MCTS uses the dynamics and prediction networks to simulate possible futures, building a search tree. (3) The agent selects actions based on the improved policy from MCTS. (4) After taking the action, the networks are updated using the actual observed outcomes. This creates a self-improving loop where better planning leads to better data, which improves the model, enabling even better planning.",
        "verification": "MuZero achieved superhuman performance on all 57 Atari games while using significantly less data than previous methods. It also matched AlphaZero's performance in Go, chess, and shogi without knowing the rules, demonstrating that it can learn effective planning models purely from experience. On Atari, it achieved a median human-normalized score of 742.6% across all games, surpassing previous model-free methods.",
        "inspiration": "AlphaZero's planning with MCTS, combined with the insight that the environment model itself can be learned rather than provided."
    }
}