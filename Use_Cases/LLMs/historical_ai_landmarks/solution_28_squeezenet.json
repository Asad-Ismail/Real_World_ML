{
    "solution_name": "SqueezeNet (SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and <0.5MB model size)",
    "simplified_problem": "Ultra-lightweight deep neural networks for resource-constrained deployment.",
    "problem_it_solved": "Deep neural networks like AlexNet achieve good accuracy on image classification tasks but require millions of parameters (e.g., AlexNet has ~60M parameters), making them unsuitable for deployment on resource-constrained devices with limited memory, storage, and computational power. The challenge is to achieve comparable accuracy while dramatically reducing model size and parameter count.",
    "historical_context": "In 2016, deep learning models were becoming increasingly large and computationally expensive. AlexNet (2012) had 60M parameters, VGGNet (2014) had 138M parameters, and even newer models were trending toward larger sizes. This created a significant gap between research models and practical deployment on mobile devices, embedded systems, and IoT applications where memory might be limited to a few MB and computational resources are constrained. The prevailing approach was to accept large models and then compress them post-training, but this added complexity and could hurt accuracy.",
    "landmark_solution_details": {
        "domain": "Deep Learning Model Compression",
        "title": "SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and <0.5MB model size",
        "concept": "Achieve dramatic model compression through three key strategies: (1) Replace 3x3 filters with 1x1 filters to reduce parameters by 9x, (2) Decrease the number of input channels to 3x3 filters to further reduce parameters, and (3) Downsample late in the network to give convolutional layers large activation maps for higher accuracy. These principles are implemented in the 'Fire module' architecture.",
        "math_foundation": "Parameter reduction is achieved through: (1) 1x1 convolutions reduce parameters from 3\u00d73\u00d7C_in\u00d7C_out to 1\u00d71\u00d7C_in\u00d7C_out (9\u00d7 reduction), (2) Squeeze layer with s1\u00d71 1\u00d71 filters reduces input channels to expand layers, (3) Total parameters in Fire module: s1\u00d71\u00d7C_in + (e1\u00d71 + e3\u00d73)\u00d7s1\u00d71, where s1\u00d71 < e1\u00d71 + e3\u00d73 for compression.",
        "implementation": "The Fire module consists of: (1) Squeeze layer with only 1\u00d71 filters, (2) Expand layer with a mix of 1\u00d71 and 3\u00d73 filters, (3) Concatenation of expand layer outputs. The full SqueezeNet architecture stacks these Fire modules with max-pooling and late downsampling, achieving AlexNet-level accuracy with 1.25M parameters (50\u00d7 fewer) and 0.5MB storage (32-bit) or 0.47MB (8-bit quantization).",
        "verification": "SqueezeNet achieved AlexNet-level accuracy on ImageNet (57.5% top-1, 80.3% top-5) while using 50\u00d7 fewer parameters (1.25M vs 60M). With Deep Compression (quantization + pruning + Huffman coding), the model size reduced to 0.47MB (510\u00d7 smaller than AlexNet) while maintaining accuracy.",
        "inspiration": "Network design principles from LeNet (small filters), Inception modules (multi-scale features), and compression techniques from signal processing."
    }
}