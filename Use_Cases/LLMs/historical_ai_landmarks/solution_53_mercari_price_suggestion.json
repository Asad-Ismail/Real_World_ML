{
    "solution_name": "Mercari Price Suggestion (Kaggle winning solution combining TF-IDF, embeddings, and LightGBM)",
    "simplified_problem": "Accurate price prediction for e-commerce listings from unstructured text and images.",
    "problem_it_solved": "How can we automatically suggest fair and accurate prices for millions of diverse e-commerce product listings when each listing contains highly variable, noisy, and incomplete information\u2014primarily free-form text descriptions, item names, and product images\u2014across hundreds of different product categories?",
    "historical_context": "In 2017, Mercari launched a Kaggle competition to build an automated price suggestion system for their Japanese C2C marketplace. At the time, existing pricing models were either rule-based (manual heuristics per category) or relied on simple regression with limited feature engineering. These approaches struggled with:\n- Extreme sparsity and noise in textual descriptions (typos, slang, mixed languages)\n- Visual diversity within categories (e.g., \"T-shirts\" ranging from $5 basics to $500 designer pieces)\n- Long-tail distribution of prices with heavy skew\n- Need to handle 1.4M+ listings across 1,300+ categories in real-time",
    "landmark_solution_details": {
        "domain": "Applied Machine Learning / E-commerce",
        "title": "Mercari Price Suggestion: Kaggle 1st Place Solution",
        "concept": "A multi-modal ensemble that combines TF-IDF text features, neural embeddings (Word2Vec + ImageNet CNN), and gradient boosting (LightGBM) to predict prices from noisy e-commerce data. The key insight was that different feature types capture complementary signals: TF-IDF for exact keyword matches, embeddings for semantic similarity, and images for visual quality/brand signals.",
        "math_foundation": "The final prediction is a weighted ensemble: \u0177 = \u03b1\u00b7\u0177_text + \u03b2\u00b7\u0177_image + \u03b3\u00b7\u0177_meta, where each component is trained via LightGBM optimizing RMSLE (Root Mean Squared Logarithmic Error). TF-IDF uses sublinear scaling: tf-idf(t,d) = tf(t,d) * log((N+1)/(df(t)+1)) + 1. Word2Vec embeddings are averaged across tokens: v_doc = (1/|tokens|) \u03a3 v_token. Image features use ResNet50's 2048-dim penultimate layer.",
        "implementation": "1) Text pipeline: TF-IDF (1-3 grams) + Word2Vec (300-dim, trained on 10M+ descriptions) concatenated \u2192 LightGBM. 2) Image pipeline: ResNet50 (pretrained on ImageNet) \u2192 Global Average Pooling \u2192 LightGBM. 3) Meta-features: category, brand, shipping, item_condition \u2192 LightGBM. 4) Stacking: 3-fold CV predictions from each model become meta-features for final LightGBM ensemble. Target transformation: log1p(price) to handle skew.",
        "verification": "Achieved 0.3905 RMSLE on private leaderboard (1st place), significantly outperforming Mercari's baseline (0.45+). Ablation showed: TF-IDF alone \u2192 0.42, adding Word2Vec \u2192 0.405, adding images \u2192 0.395, full ensemble \u2192 0.3905. The solution generalized across categories, handling both high-volume (women's fashion) and long-tail (collectibles) segments.",
        "inspiration": "Classical NLP (TF-IDF) + Modern Deep Learning (CNN embeddings) + Gradient Boosting (LightGBM) for tabular data"
    }
}