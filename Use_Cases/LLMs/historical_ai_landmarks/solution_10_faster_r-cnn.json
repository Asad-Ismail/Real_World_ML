{
    "solution_name": "Faster R-CNN (Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks)",
    "simplified_problem": "Real-time object detection with end-to-end learning.",
    "problem_it_solved": "Object detection systems required separate stages for generating region proposals (potential object locations) and classifying those regions, making them slow and suboptimal. The region proposal stage (like Selective Search) was a computational bottleneck, taking 2-3 seconds per image, preventing real-time performance and preventing joint optimization of the entire detection pipeline.",
    "historical_context": "Before Faster R-CNN, the state-of-the-art object detection pipeline was R-CNN \u2192 Fast R-CNN. R-CNN used Selective Search to generate ~2000 region proposals per image, then ran a CNN independently on each cropped region - this was extremely slow. Fast R-CNN improved this by sharing convolutional computation across the entire image and using ROI pooling, but still relied on external region proposal methods like Selective Search or EdgeBoxes. These external methods were not learned, could not be optimized for the detection task, and remained the major speed bottleneck, preventing real-time detection.",
    "landmark_solution_details": {
        "domain": "Computer Vision - Object Detection",
        "title": "Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks",
        "concept": "Replace external region proposal methods with a Region Proposal Network (RPN) that shares convolutional features with the detection network. The RPN is a fully convolutional network that simultaneously predicts object bounds and objectness scores at each spatial location, using anchor boxes of multiple scales and aspect ratios. This creates a single, unified network that can be trained end-to-end for both region proposal and object detection.",
        "math_foundation": "The RPN is trained with a multi-task loss: L({p_i}, {t_i}) = (1/N_cls) \u03a3_i L_cls(p_i, p_i*) + \u03bb(1/N_reg) \u03a3_i p_i* L_reg(t_i, t_i*). Here p_i is the predicted probability of anchor i being an object, p_i* is the ground-truth label (1 for positive, 0 for negative), t_i is the predicted bounding box regression coefficients, and t_i* is the ground-truth box. L_cls is log loss for classification, L_reg is smooth L1 loss for regression. The \u03bb parameter balances the two losses.",
        "implementation": "The RPN is built on top of the last shared convolutional layer. It uses a small network sliding over the convolutional feature map, with a 3\u00d73 spatial window followed by two sibling 1\u00d71 convolutional layers - one for box-classification (object vs. background) and one for box-regression. Anchors are generated at each sliding-window location with 3 scales and 3 aspect ratios (9 anchors total). The entire system is trained with a 4-step alternating training algorithm that first trains RPN, then trains Fast R-CNN using RPN proposals, then fine-tunes RPN with Fast R-CNN features, and finally fine-tunes Fast R-CNN with RPN proposals.",
        "verification": "Faster R-CNN achieved 73.2% mAP on PASCAL VOC 2007 test set while running at 5 fps on a GPU, compared to Fast R-CNN's 39.3% mAP at 2.3 fps (when using Selective Search). On the more challenging MS COCO dataset, it achieved 42.7% mAP, significantly outperforming previous methods while being the first to enable near real-time object detection.",
        "inspiration": "Sliding-window object detection methods and the insight that convolutional feature maps can be reused for both region proposal and classification tasks."
    }
}