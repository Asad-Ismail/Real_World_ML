{
    "solution_name": "AlphaGo (Mastering the game of Go with deep neural networks and tree search)",
    "simplified_problem": "Mastering complex strategy games beyond human-level performance.",
    "problem_it_solved": "The game of Go presents an enormous search space (approximately 10^170 possible positions) that makes traditional game-playing approaches like brute-force search or handcrafted evaluation functions infeasible. Previous AI systems could not achieve professional-level play, with the best programs playing at only amateur dan level, far below top human professionals.",
    "historical_context": "Before AlphaGo, computer Go programs had plateaued for decades. The strongest programs like Crazy Stone and Zen used Monte Carlo Tree Search (MCTS) combined with handcrafted evaluation functions and pattern databases, but could only reach amateur 6-dan level. The complexity of Go (with its 19\u00d719 board and long-term strategic planning) made it the last classic board game where humans remained dominant. Deep Blue had defeated Kasparov in chess in 1997, but Go was considered \"the holy grail of AI board games\" due to its intuitive nature and enormous branching factor (\u2248250 legal moves per position vs \u224835 in chess).",
    "landmark_solution_details": {
        "domain": "Game Playing AI",
        "title": "Mastering the game of Go with deep neural networks and tree search",
        "concept": "AlphaGo combines deep neural networks with Monte Carlo Tree Search in a novel way. It uses two deep networks: a policy network that suggests promising moves, and a value network that evaluates board positions. These networks are trained through a pipeline: supervised learning from human expert games, followed by self-play reinforcement learning where the system improves by playing millions of games against itself.",
        "math_foundation": "The system uses a variant of Monte Carlo Tree Search called 'asynchronous policy and value MCTS'. The tree search is guided by: P(s,a) = (1-\u03b5)p_\u03b8(a|s) + \u03b5\u03b7_a, where p_\u03b8 is the policy network output, \u03b7 is Dirichlet noise for exploration, and \u03b5 controls the mixing. The value function v_\u03b8(s) approximates the expected outcome from position s, trained to minimize the mean squared error between predicted and actual game outcomes.",
        "implementation": "The system consists of: (1) A 13-layer policy network trained on 30 million positions from human games, achieving 57% accuracy in predicting expert moves. (2) A value network trained on 30 million self-play positions to predict game outcomes. (3) MCTS that uses both networks: the policy network to narrow the search to high-probability moves, and the value network to evaluate leaf positions instead of random rollouts. The search tree uses UCT (Upper Confidence bounds applied to Trees) with modifications to incorporate the neural network evaluations.",
        "verification": "AlphaGo defeated European Go champion Fan Hui 5-0 in October 2015, becoming the first AI to beat a professional player without handicap. It then defeated legendary player Lee Sedol 4-1 in March 2016, a result considered shocking to the Go community. The distributed version (AlphaGo Lee) used 1,920 CPUs and 280 GPUs, while the later AlphaGo Master and AlphaGo Zero achieved even stronger performance with less hardware.",
        "inspiration": "Monte Carlo methods for game tree search, deep learning for pattern recognition, and reinforcement learning through self-play."
    }
}