{
    "solution_name": "FPN (Feature Pyramid Networks for Object Detection)",
    "simplified_problem": "Multi-scale feature representation for object detection.",
    "problem_it_solved": "Object detection requires recognizing objects at vastly different scales - from small objects like distant pedestrians to large objects like nearby trucks. Traditional approaches either used only the final layer's features (losing fine-grained details for small objects) or processed the image at multiple scales independently (computationally expensive). The challenge was to create a unified feature representation that could simultaneously capture both high-level semantic information for large objects and low-level spatial details for small objects, while maintaining computational efficiency.",
    "historical_context": "Before FPN, object detectors like Faster R-CNN used only the final convolutional layer's features for detection. This worked well for medium to large objects but struggled with small objects because the final features had low spatial resolution due to pooling operations. Alternative approaches included: (1) Image pyramids - running the detector on multiple resized versions of the image, which was computationally expensive as it required re-computing features for each scale; (2) Using features from different layers independently - but this created inconsistent representations across scales; (3) SSD used multiple feature maps from different layers but without strong connections between them, leading to semantic gaps between scales. The field needed a principled way to combine multi-scale features while preserving both semantic strength and spatial resolution.",
    "landmark_solution_details": {
        "domain": "Computer Vision - Object Detection",
        "title": "Feature Pyramid Networks for Object Detection",
        "concept": "Build a top-down architecture with lateral connections that creates a feature pyramid where each level has strong semantics (from deeper layers) and high resolution (from shallower layers). The network combines low-resolution, semantically strong features with high-resolution, semantically weak features through a top-down pathway and lateral connections, creating features rich for detection at all scales.",
        "math_foundation": "The top-down pathway upsamples spatially coarser feature maps from higher pyramid levels using nearest neighbor upsampling by a factor of 2. Lateral connections merge these upsampled features with the bottom-up pathway using element-wise addition: P_l = Conv_1x1(C_l) + Upsample(P_{l+1}), where C_l are features from the bottom-up pathway at layer l, and P_l are the final pyramid features. A 3x3 convolution is applied after merging to reduce aliasing effects.",
        "implementation": "Start with a standard bottom-up ConvNet (e.g., ResNet). For each spatial resolution in the bottom-up pathway (e.g., {C2, C3, C4, C5}), apply a 1x1 convolution to reduce channel dimension. Then build the top-down pathway by upsampling coarser pyramid levels and merging with corresponding bottom-up features via element-wise addition. The final pyramid has levels {P2, P3, P4, P5, P6} where P6 is obtained via stride-2 subsampling on P5. Each level is used to detect objects of different scales.",
        "verification": "FPN with Faster R-CNN achieved state-of-the-art results on COCO detection, improving AP from 33.8% to 36.2% over the baseline. Notably, small object AP improved by 5.6 points (from 18.2% to 23.8%), demonstrating the effectiveness of high-resolution, semantically strong features. The method also proved general, improving RetinaNet, Mask R-CNN, and other detectors.",
        "inspiration": "Image pyramids in classical computer vision and the observation that features from different layers in CNNs naturally form a feature hierarchy."
    }
}