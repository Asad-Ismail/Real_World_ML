{
    "solution_name": "EfficientNet (EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks)",
    "simplified_problem": "Balanced scaling of CNN architectures for better accuracy-efficiency trade-offs.",
    "problem_it_solved": "Traditional approaches to improving CNN performance focused on scaling individual dimensions (depth, width, or input resolution) arbitrarily, leading to suboptimal accuracy-efficiency trade-offs. There was no principled method to determine how to scale networks while maintaining computational efficiency.",
    "historical_context": "Prior to 2019, CNN scaling followed heuristic practices: researchers would make networks deeper (ResNet-50 \u2192 ResNet-152), wider (WRN), or use higher resolution inputs independently. MobileNetV1 scaled width multipliers, while Inception-v4 increased both depth and width. However, these approaches were empirical - scaling any single dimension arbitrarily led to diminishing returns or excessive computational cost. The community lacked a systematic understanding of how depth, width, and resolution interact to determine model capacity and efficiency.",
    "landmark_solution_details": {
        "domain": "Computer Vision Architecture Design",
        "title": "EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks",
        "concept": "Introduces compound scaling - a principled method to uniformly scale network width, depth, and resolution with a fixed set of scaling coefficients. Rather than scaling dimensions independently, the method uses a compound coefficient \u03c6 to scale all dimensions in a balanced way: depth \u221d \u03c6^\u03b1, width \u221d \u03c6^\u03b2, resolution \u221d \u03c6^\u03b3, where \u03b1, \u03b2, \u03b3 are determined through neural architecture search.",
        "math_foundation": "The compound scaling method is derived from the observation that network capacity is approximately proportional to (d \u00d7 w\u00b2 \u00d7 r\u00b2), where d=depth, w=width, r=resolution. By constraining \u03b1\u00b7\u03b2\u00b2\u00b7\u03b3\u00b2 \u2248 2, the total FLOPS increase by approximately 2^\u03c6. The optimal coefficients (\u03b1=1.2, \u03b2=1.1, \u03b3=1.15 for EfficientNet-B0) are found through grid search on the baseline network.",
        "implementation": "First, a baseline network (EfficientNet-B0) is designed using neural architecture search with mobile inverted bottleneck convolutions (MBConv) and squeeze-and-excitation blocks. Then, compound scaling is applied: for EfficientNet-B1 to B7, the compound coefficient \u03c6 ranges from 1 to 2.5, with corresponding increases in depth, width, and resolution. The scaling maintains the same channel ratios and layer patterns as the baseline.",
        "verification": "EfficientNet-B7 achieved 84.3% top-1 accuracy on ImageNet with 66M parameters and 37B FLOPs, outperforming GPipe (84.3% with 557M parameters) while being 8.4x smaller. EfficientNet-B0 achieved 77.1% accuracy with only 5.3M parameters, significantly better than MobileNetV2 (72.0% with 3.4M parameters) and ResNet-50 (76.0% with 25.6M parameters).",
        "inspiration": "Neural Architecture Search (for baseline design) and control theory (for systematic scaling relationships)."
    }
}