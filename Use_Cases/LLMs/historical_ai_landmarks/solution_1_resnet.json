{
    "solution_name": "ResNet (Deep Residual Learning for Image Recognition)",
    "simplified_problem": "Training very deep neural networks without degradation.",
    "problem_it_solved": "As neural networks become deeper, they suffer from a degradation problem: adding more layers leads to higher training error (and consequently higher test error), even when the additional layers are implemented as identity mappings. This is not caused by overfitting, but by optimization difficulties where deeper networks become harder to train effectively.",
    "historical_context": "Following the breakthrough of AlexNet in 2012, the computer vision community pursued deeper architectures as a path to better performance. VGGNet demonstrated that depth could improve accuracy, but when researchers attempted to build even deeper networks (56+ layers), they observed a counterintuitive phenomenon: these deeper networks achieved higher training error than their shallower counterparts. This degradation problem couldn't be explained by overfitting, as it occurred even on the training set. The issue was that optimizing deeper networks became increasingly difficult, with gradients vanishing as they propagated through many layers. Traditional solutions like careful initialization, batch normalization, and gradient clipping helped but didn't fundamentally solve the degradation problem.",
    "landmark_solution_details": {
        "domain": "Deep Learning Optimization",
        "title": "ResNet: Deep Residual Learning for Image Recognition",
        "concept": "Instead of learning the underlying mapping H(x) directly, reformulate layers to learn a residual mapping F(x) = H(x) - x. The original mapping becomes H(x) = F(x) + x, implemented through shortcut connections that skip one or more layers. This allows gradients to flow directly through identity connections, making it easier to optimize deeper networks.",
        "math_foundation": "The residual block computes y = F(x, {W_i}) + x, where F represents the stacked layers and {W_i} are their parameters. During backpropagation, the gradient with respect to the input is: \u2202L/\u2202x = \u2202L/\u2202y * (\u2202F/\u2202x + I), where I is the identity matrix. The identity term ensures that gradients can propagate back without vanishing, as the gradient signal always has a direct path through the shortcut connection.",
        "implementation": "ResNet introduces 'bottleneck' residual blocks consisting of 1x1, 3x3, and 1x1 convolutions. When dimensions change between input and output, the shortcut connection uses a 1x1 convolution to match dimensions. The architecture stacks these blocks in stages, with downsampling performed by convolutions with stride 2. Batch normalization and ReLU are applied before convolutions (pre-activation variant).",
        "verification": "ResNet-152 achieved 3.57% top-5 error on ImageNet, winning ILSVRC 2015. It demonstrated that networks could be scaled to 1000+ layers without degradation, with ResNet-1001 achieving better performance than ResNet-152. The architecture enabled training of networks up to 152 layers (8x deeper than VGG) while improving accuracy.",
        "inspiration": "Highway Networks (gated shortcut connections) and the observation that identity mappings are easier to learn than arbitrary transformations."
    }
}