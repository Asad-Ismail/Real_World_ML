{
    "solution_name": "Backpropagation (Learning Representations by Back-propagating Errors)",
    "simplified_problem": "Efficient gradient computation for neural networks.",
    "problem_it_solved": "Training multi-layer neural networks requires computing how changes in each weight affect the overall error (the gradient), but direct computation of these gradients for networks with many layers is computationally intractable due to the exponential growth in possible paths through the network.",
    "historical_context": "In the early 1980s, neural networks were limited to shallow architectures (typically 1-2 hidden layers) because there was no efficient way to train deeper networks. Researchers knew that adjusting weights based on their contribution to the error was necessary, but computing these contributions required either brute-force perturbation (which scales linearly with the number of weights) or symbolic differentiation of the entire network function (which becomes infeasible for complex networks). The perceptron learning rule worked for single-layer networks, and the Widrow-Hoff rule could handle linear networks, but neither could train the multi-layer networks needed for complex pattern recognition tasks.",
    "landmark_solution_details": {
        "domain": "Neural Network Optimization",
        "title": "Learning Representations by Back-propagating Errors",
        "concept": "A systematic method for computing gradients in multi-layer networks by applying the chain rule recursively, working backwards from the output layer to the input layer. The key insight is that once the error at the output is known, we can efficiently compute how much each weight contributed to this error by propagating the error signal backwards through the network, reusing computations at each layer.",
        "math_foundation": "For a network with L layers, the gradient of the loss L with respect to weights W^l in layer l is computed as: \u2202L/\u2202W^l = \u03b4^l \u00b7 (a^{l-1})^T, where \u03b4^l = (W^{l+1})^T \u03b4^{l+1} \u2299 f'(z^l). Here, \u03b4^l is the error signal at layer l, a^{l-1} are activations from the previous layer, \u2299 denotes element-wise multiplication, and f' is the derivative of the activation function. This recursive application of the chain rule allows computing all gradients in O(n) time where n is the number of weights.",
        "implementation": "The algorithm proceeds in two phases: (1) Forward pass: propagate input through the network to compute activations at each layer and the final output. (2) Backward pass: compute the error at the output layer, then iteratively compute \u03b4^l for each layer from L down to 1, using the stored activations from the forward pass. Weight updates are then computed using these gradients with any optimization algorithm (typically gradient descent).",
        "verification": "The paper demonstrated successful training of networks with multiple hidden layers (up to 5 layers) on tasks like the XOR problem, symmetry detection, and character recognition, achieving performance that was impossible with single-layer networks. The method scaled to networks with thousands of weights, enabling the practical training of deep architectures.",
        "inspiration": "Optimal Control Theory (specifically, the method of Lagrange multipliers and adjoint equations used in control theory for sensitivity analysis)."
    }
}