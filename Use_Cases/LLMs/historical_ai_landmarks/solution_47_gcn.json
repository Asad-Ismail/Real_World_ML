{
    "solution_name": "GCN (Semi-Supervised Classification with Graph Convolutional Networks)",
    "simplified_problem": "Learning on irregular graph-structured data",
    "problem_it_solved": "Traditional neural networks like CNNs and RNNs are designed for regular grid-like data (images, sequences) but cannot directly handle graph-structured data where nodes have varying numbers of neighbors and no fixed spatial ordering. This prevents effective learning on social networks, molecular structures, knowledge graphs, and other domains where relationships are naturally represented as graphs.",
    "historical_context": "Before 2017, applying neural networks to graphs required either: (1) extracting fixed-size features from graph structures (losing relational information), (2) using kernel methods that couldn't scale to large graphs, or (3) applying recurrent neural networks to random walks (losing global structure). The few existing neural approaches like Duvenaud et al.'s neural fingerprints or Atwood & Towsley's diffusion-convolutional networks were either limited to small graphs or couldn't effectively capture multi-hop neighborhood information. There was no principled way to perform convolution-like operations on graphs that could leverage both node features and graph structure while being scalable to large graphs.",
    "landmark_solution_details": {
        "domain": "Graph Neural Networks",
        "title": "Semi-Supervised Classification with Graph Convolutional Networks",
        "concept": "Generalize the concept of convolution from regular grids to arbitrary graphs by defining a spectral graph convolution operation. This is approximated using a localized first-order approximation that aggregates information from a node's immediate neighborhood, creating a layer-wise propagation rule that can be stacked to capture multi-hop relationships.",
        "math_foundation": "The spectral graph convolution is defined in the Fourier domain as: g_\u03b8 * x = U g_\u03b8(\u039b) U^T x, where U contains the eigenvectors of the normalized graph Laplacian L = I - D^{-1/2}AD^{-1/2}. To avoid expensive eigendecomposition, they use a first-order approximation: g_\u03b8 * x \u2248 \u03b8(I + D^{-1/2}AD^{-1/2})x. With normalization and renormalization tricks, the layer-wise propagation becomes: H^{(l+1)} = \u03c3(\u00c3 H^{(l)} W^{(l)}), where \u00c3 = D\u0303^{-1/2}\u00c3D\u0303^{-1/2} is the normalized adjacency matrix with self-loops.",
        "implementation": "The GCN consists of multiple graph convolutional layers. Each layer takes node features H^{(l)} and the graph structure (adjacency matrix A) as input, applies the propagation rule, and outputs transformed features H^{(l+1)}. For semi-supervised node classification, a two-layer GCN is typically used: Z = softmax(\u00c3 ReLU(\u00c3 X W^{(0)}) W^{(1)}), where only a subset of nodes have known labels during training.",
        "verification": "On the Cora, Citeseer, and Pubmed citation networks, GCN achieved 81.5%, 70.3%, and 79.0% accuracy respectively with only 5.2%, 3.6%, and 0.3% of nodes labeled for training - significantly outperforming previous semi-supervised methods like label propagation and manifold regularization.",
        "inspiration": "Spectral Graph Theory (Chebyshev polynomials for spectral filters) and CNNs (localized convolutional filters)"
    }
}